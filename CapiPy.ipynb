{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLNgVSKEzT+VNgW/wqIUVT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drou0302/CapiPy/blob/main/CapiPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">**WELCOME TO CAPIPY COLAB!**\n",
        "---\n",
        "\n",
        "In this colab file, you will be able to run the three most important modules of CapiPy and directly view your results. \n",
        "At the end, there is also the possibilty for you to download your project file to your local computer. \n",
        "All functionalities and files remain the same formatting (see https://github.com/drou0302/CapiPy for more information). \n",
        "\n",
        "The three modules that can be run are:\n",
        "\n",
        "1.   MODELLER & 1.1 Quaternary model\n",
        "2.   ACTIVE SITE IDENTIFICATION\n",
        "3.   SURFACE ANALYSIS\n",
        "\n",
        "To run each of the cells, just press the play button, wait and enjoy!\n",
        "\n",
        "In the different modules, you will be asked to provide full paths to different files or folders. To do so, on the left hand side you have the access to your folder indicated by the 📁 symbol. By default, all the results are saved in the /content/ folder. To copy a path, select the file or folder and right click, then select 'Copy path' and paste directly that information. \n",
        "\n",
        "A new folder for each of your projects will be created when you run the MODELLER module. \n",
        "\n",
        "**Remember, that CapiPy is thought to be used sequentially, so follow the whole process for optimal results.**"
      ],
      "metadata": {
        "id": "ys36wXHl5Dec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DEPENDENCIES INSTALLATION\n",
        "#@markdown Download all necessary dependencies for CapiPy and install them.\n",
        "print('Downloading conda...')\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "6UXyYXUzFkku",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Installing modeller...')\n",
        "!conda install -c salilab modeller 2>&1 1>/dev/null\n",
        "\n",
        "#@markdown Enter here your modeller license. You can obtain it from https://salilab.org/modeller/registration.html\n",
        "\n",
        "MODELLER_LICENSE = \"MODELIRANJE\" #@param {type:\"string\"}\n",
        "\n",
        "%cd /usr/local/lib/modeller-10.4/modlib/modeller/\n",
        "with open('config.py', 'r+') as orig_config:\n",
        "    with open('config1.py', 'w+') as new_config:\n",
        "      for line in orig_config.readlines():\n",
        "        if 'license' in line:\n",
        "          new_config.write(\"license = r'\" + MODELLER_LICENSE + \"'\")\n",
        "        elif 'license' not in line:\n",
        "          new_config.write(line)\n",
        "%rm config.py\n",
        "%mv config1.py config.py\n",
        "%cd /content/\n",
        "\n",
        "print('Installing biopython...')\n",
        "!pip install biopython 2>&1 1>/dev/null\n",
        "!pip install xmltramp2 2>&1 1>/dev/null\n",
        "!pip install pandas 2>&1 1>/dev/null\n",
        "!pip install py3Dmol 2>&1 1>/dev/null\n",
        "!wget https://raw.githubusercontent.com/ebi-wp/webservice-clients/master/python/ncbiblast.py 2>&1 1>/dev/null\n",
        "\n",
        "\n",
        "print('Installing clustalw2...') \n",
        "!wget http://www.clustal.org/download/current/clustalw-2.1.tar.gz 2>&1 1>/dev/null\n",
        "!tar xvzf clustalw-2.1.tar.gz 2>&1 1>/dev/null\n",
        "%cd clustalw-2.1 \n",
        "! ./configure 2>&1 1>/dev/null\n",
        "!make &> /dev/null\n",
        "!make install &> /dev/null\n",
        "%cd /content/\n",
        "!rm clustalw-2.1.tar.gz\n",
        "\n",
        "print('Dependencies created!')"
      ],
      "metadata": {
        "id": "F6NBvvrazJ-T",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title BLAST local installation - OPTIONAL AND NOT RECOMMENDED... UNLESS YOU HAVE A LOT OF TIME #\n",
        "#@markdown Only if you really want to use the local blast. This can take a long time!\n",
        "\n",
        "\n",
        "print('This might take a few minutes... so take a coffe and relax!')\n",
        "\n",
        "print('Installing BLAST...')\n",
        "!wget https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.13.0+-src.tar.gz \n",
        "!mkdir blast 2>&1 1>/dev/null\n",
        "!tar -xf ncbi-blast-2.13.0+-src.tar.gz 2>&1 1>/dev/null\n",
        "!./ncbi-blast-2.13.0+-src/c++/configure 2>&1 1>/dev/null\n",
        "!cd /content/ncbi-blast-2.13.0+-src/c++/ReleaseMT/build && /usr/bin/make all_r 2>&1 1>/dev/null\n",
        "\n",
        "print('Creating the BLAST database...')\n",
        "!wget https://ftp.ncbi.nlm.nih.gov/blast/db/swissprot.tar.gz \n",
        "!wget https://ftp.ncbi.nlm.nih.gov/blast/db/pdbaa.tar.gz \n",
        "\n",
        "!mkdir db 2>&1 1>/dev/null\n",
        "!mkdir ./db/swissprot 2>&1 1>/dev/null\n",
        "!mkdir ./db/pdbaa 2>&1 1>/dev/null\n",
        "!tar -xf swissprot.tar.gz --directory ./db/swissprot 2>&1 1>/dev/null\n",
        "!tar -xf pdbaa.tar.gz --directory ./db/pdbaa 2>&1 1>/dev/null\n"
      ],
      "metadata": {
        "id": "r9C3eXnkzgF9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import dependencies\n",
        "#@markdown Import all necessary dependencies to run the CapiPy modules\n",
        "import urllib.request\n",
        "import urllib.parse\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import numpy\n",
        "import time\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import traceback\n",
        "import requests\n",
        "import csv\n",
        "import re\n",
        "import py3Dmol\n",
        "\n",
        "from Bio.Blast import NCBIWWW\n",
        "from Bio.Blast.Applications import NcbiblastpCommandline\n",
        "from Bio import SeqIO, AlignIO, SearchIO, Align\n",
        "from Bio.PDB import is_aa, PDBList, PDBIO, Superimposer\n",
        "from Bio.PDB.Selection import unfold_entities\n",
        "from Bio.PDB.PDBParser import PDBParser\n",
        "from Bio.PDB.Polypeptide import *\n",
        "from modeller import *\n",
        "from modeller.automodel import *\n",
        "from sys import platform\n",
        "from Bio import BiopythonWarning\n",
        "from Bio.Blast import NCBIXML\n",
        "from Bio.Align import substitution_matrices\n",
        "from Bio.Align.Applications import ClustalwCommandline\n",
        "from Bio import Entrez, Seq, SeqIO, SearchIO\n",
        "clustalw_exe = 'clustalw2'"
      ],
      "metadata": {
        "id": "QtfJiXtUJT0F",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODULE 1 - MODELLER\n",
        "%cd /content\n",
        "#@markdown Indicate your email address. This is for uniprot retreival and no data is saved.\n",
        "EMAIL_ADDRESS = \"davidrourap@gmail.com\" #@param {type:\"string\"}\n",
        "#@markdown Indicate the project name you want to give. A new folder will be created in /content/ with that name and will contain the run of all modules. \n",
        "PROJECT_NAME = \"TEST\" #@param {type:\"string\"}\n",
        "#@markdown Enter the 1-letter amino acid code of your protein. Only amino acid letters are allowed. \n",
        "PROTEIN_SEQUENCE = \"MDIGINSDPQTQDYQALDRAHHLHPFTDFKALGEEGSRVVTHAEGVYIHDSEGNRILDGMAGLWCVNLGYGRRELVEAATAQLEQLPYYNTFFKTTHPPAVRLAEKLCDLAPAHINRVFFTGSGSEANDTVLRMVRRYWALKGQPDKQWIIGRENAYHGSTLAGMSLGGMAPMHAQGGPCVPGIAHIRQPYWFGEGRDMSPEAFGQTCAEALEEKILELGEEKVAAFIAEPVQGAGGAIMPPESYWPAVKKVLAKYDILLVADEVICGFGRLGEWFGSQHYGLEPDLMPIAKGLSSGYLPIGGVLVGDRVAETLIEEGGEFFHGFTYSGHPTCAAVALKNLELLEAEGVVDRVRDDLGPYLAERWASLVDHPIVGEARSLGLMGALELVADKTTGQRFDKSLGAGNLCRDLCFANGLVMRSVGDTMIISPPLVIRREEIDELVELARRALDETARQLTQVPHTQEEPTAKLAAAENLYFQGLEHHHHHH\" #@param {type:\"string\"}\n",
        "#@markdown Indicate if you'd want to use the alphafold structure or a PDB structure to model your protein.\n",
        "MODELLING = \"ALPHAFOLD\" #@param [\"PDB\", \"ALPHAFOLD\"]\n",
        "\n",
        "#@markdown Ready to go! Press play. This can take up to 10 minutes.\n",
        "\n",
        "initial_location = os.getcwd()\n",
        "working_directory = PROJECT_NAME\n",
        "if os.path.exists(\"./\" + working_directory) is False:\n",
        "   os.mkdir(\"./\" + working_directory)\n",
        "   print(\"Directory \", working_directory, \" created.\")\n",
        "   os.chdir(\"./\" + working_directory)\n",
        "else:\n",
        "   os.chdir(\"./\" + working_directory)\n",
        "if os.path.exists(\"./\" + \"Modeller\") is False:\n",
        "    os.mkdir(\"Modeller\")\n",
        "\n",
        "os.chdir(\"./Modeller\")\n",
        "\n",
        "print('Finding the best hit from UniProt...')\n",
        "!python /content/ncbiblast.py --quiet --email=$EMAIL_ADDRESS --stype=protein --program=blastp --database=uniprotkb_refprotswissprot --outfile=uniprot_blast_result --sequence=$PROTEIN_SEQUENCE\n",
        "upblastfiles = glob.glob('uniprot_blast*')\n",
        "upblastfiles.remove('uniprot_blast_result.out.txt')\n",
        "for file in upblastfiles:\n",
        "    os.remove(file)\n",
        "!mv uniprot_blast_result.out.txt uniprot_blast_result.txt  \n",
        "\n",
        "\n",
        "\n",
        "def main(PROJECT_NAME, PROTEIN_SEQUENCE, MODELLING):\n",
        "    protein_sequence = PROTEIN_SEQUENCE\n",
        "    with open(\"query.fasta\", \"w+\") as f:\n",
        "        f.write(\">query\" + \"\\n\" + protein_sequence)\n",
        "\n",
        "    #Eliminate the his tag from the search to avoid partial hits with no real \n",
        "    #significance#\n",
        "\n",
        "    if \"HHHHH\" in protein_sequence[0:20]:\n",
        "        with open(\"protein_blast.fasta\", \"w+\") as f:\n",
        "            f.write(\">query\\n\" + protein_sequence[20:])\n",
        "    elif \"HHHHH\" in protein_sequence[len(protein_sequence) - 20:len(protein_sequence)]:\n",
        "        with open(\"protein_blast.fasta\", \"w+\") as f:\n",
        "            f.write(\">query\\n\" + protein_sequence[0:len(protein_sequence) - 20])\n",
        "    else:\n",
        "        with open(\"protein_blast.fasta\", \"w+\") as f:\n",
        "            f.write(\">query\" + \"\\n\" + protein_sequence)\n",
        "\n",
        "    # Identification of the best hit from the PDB database using query.fasta as query in blast\n",
        "    template_modelling = MODELLING\n",
        "    query = SeqIO.read(open(\"protein_blast.fasta\"), format=\"fasta\")\n",
        "    print(\"Finding the best hit from the PDB...\")\n",
        "    pdb_blast = NCBIWWW.qblast(\"blastp\",\n",
        "                               \"pdbaa\",\n",
        "                                query,\n",
        "                                auto_format='XML')\n",
        "    with open('pdb_blast_result.xml', 'w') as out_handle:\n",
        "        out_handle.write(pdb_blast.read())\n",
        "    pdb_blast.close()\n",
        "   \n",
        "    blast_pdb_result = SearchIO.read(\"pdb_blast_result.xml\", \"blast-xml\") \n",
        "    if len(blast_pdb_result) > 0:\n",
        "        blast_qresult = SearchIO.read(\"pdb_blast_result.xml\", \"blast-xml\")[0]\n",
        "        best_hit = blast_qresult[0]\n",
        "        pdb_id = best_hit.hit.id.split(\"|\")[1]\n",
        "        print(\"Your blast returned the following as the best hit:\\n\" + str(pdb_id)\n",
        "          + \".\\n Visit https://www.rcsb.org/structure/\" + str(pdb_id) + \" for more information about the model.\")\n",
        "\n",
        "\n",
        "    else:\n",
        "      print(\"Your blast doesn't seem to contain any result. Check the sequence and run the program again. \"\n",
        "          \"If you run it over the server, consider using local blast. The program is going to quit now.\")\n",
        "\n",
        "\n",
        "    parser = PDBParser()\n",
        "    ppb = PPBuilder()\n",
        "    pdbl = PDBList()\n",
        "    if MODELLING == \"ALPHAFOLD\":\n",
        "        #Find and download the Alphafold prediction from Uniprot\n",
        "        print('Downloading AlphaFold prediction for your protein...')       \n",
        "        uniprot_blast = SearchIO.read('uniprot_blast_result.txt', 'blast-text')       \n",
        "        if len(uniprot_blast) > 0:\n",
        "            identifier = uniprot_blast[0].id.split(':')[1]\n",
        "            urllib.request.urlretrieve('https://alphafold.ebi.ac.uk/files/AF-' + identifier + '-F1-model_v4.pdb',\n",
        "                                  './' +identifier +'.pdb')\n",
        "            print('Done! Your protein has the UniProt number: ' + str(identifier))\n",
        "            \n",
        "            model_pdbfile = identifier +'.pdb'\n",
        "            model_file_name = str(identifier)\n",
        "            model_structure = parser.get_structure(model_file_name, model_pdbfile)\n",
        "            model_chain = model_structure.get_chains() \n",
        "        else:\n",
        "            template_modelling = 'PDB'\n",
        "    elif template_modelling == 'PDB':\n",
        "        print('No hit was found in Uniprot... Changing to MODELLER.')\n",
        "        pdbl.retrieve_pdb_file(pdb_id, pdir='.', file_format='pdb')\n",
        "        model_pdbfile = 'pdb' + pdb_id.casefold() + '.ent'\n",
        "        model_file_name = str(pdb_id)\n",
        "        model_structure = parser.get_structure(model_file_name, model_pdbfile)\n",
        "        model_chain = model_structure.get_chains()\n",
        "\n",
        "    # Extract first hit and create a fasta with its pdb identifier and its sequence\n",
        "    # Method for alignment with modeller. The option determines if only aa are used for the alignment or not.\n",
        "    # This helps for enzymes with discontinuities and internal cofactors such as MIO dependant enzymes.\n",
        "    def align_for_modeller(OPTION):\n",
        "        for chain in model_structure.get_chains():\n",
        "            last_chain = chain.id\n",
        "            tnum_res = (len([_ for _ in chain.get_residues() if is_aa(_)]))\n",
        "        for pp in ppb.build_peptides(model_structure[0][str(last_chain)], aa_only=OPTION):\n",
        "            with open(\"model.fasta\", \"a\") as f:\n",
        "                sequence = pp.get_sequence()\n",
        "                f.write(str(sequence))\n",
        "        mdel = model_structure[0]\n",
        "        chain = mdel[last_chain]\n",
        "        res_list = unfold_entities(chain, \"R\")\n",
        "        for residue in res_list:\n",
        "            nid_1aa = (res_list[0].get_id())[1]\n",
        "            nid_lastaa = (res_list[tnum_res - 1].get_id())[1]\n",
        "        # Modify files so they have only 1 identifier + seq (id_1 or query)#\n",
        "        with open('model.fasta', 'r') as o:\n",
        "            data = o.read()\n",
        "        with open('model.fasta', 'w') as mo:\n",
        "            mo.write(\">\" + model_file_name + \"\\n\" + data)\n",
        "        with open(\"query.fasta\") as o2:\n",
        "            line2 = o2.readlines()\n",
        "        line2[0] = \">query\\n\"\n",
        "        with open(\"query.fasta\", \"w\") as m1:\n",
        "            m1.writelines(line2)\n",
        "            # Combine original sequence and first hit into a fasta input file for the alignment#\n",
        "        filenames = [\"query.fasta\", \"model.fasta\"]\n",
        "        with open('alignment.fasta', 'w+') as aligninput:\n",
        "            for files in filenames:\n",
        "                with open(files) as infile:\n",
        "                    aligninput.write(infile.read())\n",
        "                aligninput.write(\"\\n\")\n",
        "                # Profile-profile alignment using salign from modeller#\n",
        "        log.none()\n",
        "        aln = alignment(env, file='alignment.fasta', alignment_format='FASTA')\n",
        "        aln.salign(rr_file='${LIB}/blosum62.sim.mat', gap_penalties_1d=(-500, 0), output='', align_block=15,\n",
        "                    align_what='PROFILE', alignment_type='PAIRWISE', comparison_type='PSSM', similarity_flag=True,\n",
        "                    substitution=True, smooth_prof_weight=10.0)\n",
        "        aln.write(file='salign.ali', alignment_format='PIR')\n",
        "        print(\"Alignment of template and query for modelling successfull.\")\n",
        "        # Fix formatting of the .ali file to specify the 1st and last aminoacid and the structure of the model protein#\n",
        "        shutil.copyfile(\"salign.ali\", \"salign1.ali\")\n",
        "        with open(\"salign1.ali\", \"r\") as file:\n",
        "            filedata = file.read()\n",
        "        replacement = filedata.replace(\">P1;\" + str(model_file_name) + \"\\nsequence::: :: :::-1.00:-1.00\",\n",
        "                                        \">P1;\" + str(model_file_name) + \"\\nstructure\" + \":\" + str(model_file_name) + \":\" + str(\n",
        "                                            nid_1aa) + \":\" + str(last_chain) + \":\" + str(nid_lastaa) + \": ::::\")\n",
        "        with open(\"salign1.ali\", \"w+\") as f:\n",
        "            f.write(replacement)\n",
        "        # Make a single model of the query sequence using: salign1.ali and model.pdb#\n",
        "        a = automodel(env, alnfile=\"salign1.ali\", knowns=model_file_name, sequence=\"query\")\n",
        "        a.starting_model = 1\n",
        "        a.ending_model = 1\n",
        "        a.make()\n",
        "        # Check how good the alignment is#\n",
        "        pir_alignment = AlignIO.read(\"salign1.ali\", \"pir\")\n",
        "        total_length = len(pir_alignment[0])\n",
        "        gaps_1 = 0\n",
        "        gaps_2 = 0\n",
        "        for aas in pir_alignment[0].seq:\n",
        "            if aas == \"-\":\n",
        "                gaps_1 += 1\n",
        "        for aas in pir_alignment[1].seq:\n",
        "            if aas == \"-\":\n",
        "                gaps_2 += 1\n",
        "        if gaps_1 / total_length > 0.5 or gaps_2 / total_length > 0.5:\n",
        "            print(\n",
        "                \"\\nYour model protein covers less than half of the query. The created model could be inaccurate, \" +\n",
        "                \"please check the result before continuing with anything else.\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Creating your model with Modeller...')\n",
        "    env = Environ()        \n",
        "    # Run the previous method, changing the option if it fails the first time. If non of them work,\n",
        "    # raise and error and quit the program.      \n",
        "    try:\n",
        "        align_for_modeller(True)\n",
        "        print(\"First model created succesfully\")\n",
        "\n",
        "    except BaseException:\n",
        "        try:\n",
        "            delete_files = [\"alignment.fasta\", \"salign.ali\", \"salign1.ali\", \"model.fasta\"]\n",
        "            for files in delete_files:\n",
        "                if os.path.exists(files):\n",
        "                    os.remove(files)\n",
        "            align_for_modeller(False)\n",
        "        except BaseException:\n",
        "            print(traceback.format_exc())\n",
        "            delete_files = [\"salign.ali\", \"salign1.ali\", \"model.fasta\"]\n",
        "            for files in delete_files:\n",
        "                if os.path.exists(files):\n",
        "                    os.remove(files)\n",
        "            print(\n",
        "                \"\\nThere is problem with your model. It could be related to unnatural amino acids or problems in \"\n",
        "                \"the residue numbering. Please, retry with a different PDB or try and fix the errors in the file.\"\n",
        "                \"\\nThe program is now going to quit. Sorry!\")\n",
        "\n",
        "\n",
        "    # Method to superimpose the chains#\n",
        "    def superimpose_chains(dict):\n",
        "        checks = {}\n",
        "        # Check if clustalw is available.\n",
        "        for k_chain in toalign.keys():\n",
        "            chainmodel = parser.get_structure(\"CH0\" + toalign[k_chain], \"chain\" + toalign[k_chain] + \".pdb\")\n",
        "            chainquery = parser.get_structure(\"Q00\" + str(k_chain), \"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "            ppb = PPBuilder()\n",
        "            # To maximise the accuracy of the superimposer and avoid problems with alignments of low quality instead\n",
        "            # of using the whole protein to superimpose, two regions at the start and end will be used. To do so, first\n",
        "            # it's necessary to identify two regions at the start and end where both there are no gaps, checking the\n",
        "            # alignment for a minimum of 40 positions without \"-\" in reading frames of 5 aminoacids.\n",
        "            frac_chainmodelseq = []\n",
        "            chainmodelseq = \"\"\n",
        "            for pp in ppb.build_peptides(chainmodel):\n",
        "                frac_chainmodelseq.append(pp.get_sequence())\n",
        "            for fractions in frac_chainmodelseq:\n",
        "                chainmodelseq += fractions\n",
        "            for pp in ppb.build_peptides(chainquery):\n",
        "                chainqueryseq = pp.get_sequence()\n",
        "            with open(\"initial\" + str(k_chain) + \".fasta\", \"w\") as input_file:\n",
        "                input_file.write(\n",
        "                    \">model_\" + toalign[k_chain] + \"\\n\" + str(chainmodelseq) + \"\\n>query_\" + str(k_chain) + \"\\n\" + str(\n",
        "                        chainqueryseq))\n",
        "\n",
        "            ex_clustal = ClustalwCommandline(clustalw_exe, infile=\"initial\" + str(k_chain) + \".fasta\")\n",
        "            stdout, stderr = ex_clustal()\n",
        "            alignment = AlignIO.read(\"initial\" + str(k_chain) + \".aln\", \"clustal\")\n",
        "            length = len(alignment[0])\n",
        "            # Identify the first region of 40 aa without gaps in the alignment\n",
        "            i = 0\n",
        "            posm = 0\n",
        "            posq = 0\n",
        "            posmf = 0\n",
        "            posqf = 0\n",
        "            for a in range(int(length / 5)):\n",
        "                if \"-\" not in alignment[0][i:i + 40].seq and \"-\" not in alignment[1][i:i + 40]:\n",
        "                    posq = i\n",
        "                    posm = i\n",
        "                else:\n",
        "                    i += 5\n",
        "            # Translate the alignment position to the real position in each sequence\n",
        "            for aa in alignment[0].seq[0:i]:\n",
        "                if aa == \"-\":\n",
        "                    posm -= 1\n",
        "            for aa in alignment[1].seq[0:i]:\n",
        "                if aa == \"-\":\n",
        "                    posq -= 1\n",
        "            # Identify in the last quarter of the the alignment, a region of 40 aa without gaps\n",
        "            k = length - (length // 4)\n",
        "            while k < length - 40:\n",
        "                if \"-\" not in alignment[0][k:k + 40].seq and \"-\" not in alignment[1][k:k + 40].seq:\n",
        "                    posqf = k\n",
        "                    posmf = k\n",
        "                    break\n",
        "                else:\n",
        "                    k += 5\n",
        "            if k + 40 > length:\n",
        "                k = length - (length // 4)\n",
        "                while k < length - 40:\n",
        "                    if \"-\" not in alignment[0][k:k + 40].seq and \"-\" not in alignment[1][k:k + 40].seq:\n",
        "                        posqf = k\n",
        "                        posmf = k\n",
        "                        break\n",
        "                    else:\n",
        "                        k += 2\n",
        "            if posqf == 0:\n",
        "                posqf = length - (length // 4)\n",
        "                posmf = length - (length // 4)\n",
        "            # Translate the alignment position to the real position in each sequence\n",
        "            gapsmodel = 0\n",
        "            for aa in alignment[0].seq[0:k]:\n",
        "                if aa == \"-\":\n",
        "                    gapsmodel += 1\n",
        "                    posmf -= 1\n",
        "            for aa in alignment[1].seq[0:k]:\n",
        "                if aa == \"-\":\n",
        "                    posqf -= 1\n",
        "            # Check if the pdb file starts at amino acid 1 or not. If not, fix the range to be applied to the model.\n",
        "            mdel_res_list = unfold_entities(chainmodel, \"R\")\n",
        "            first_aa_model = mdel_res_list[0].get_id()[1]\n",
        "            if first_aa_model != 1:\n",
        "                posm = posm + first_aa_model\n",
        "                posmf = posmf + first_aa_model\n",
        "\n",
        "            discont = []\n",
        "            for res in mdel_res_list:\n",
        "                if is_aa(res) is True:\n",
        "                    discont.append(res.get_id()[1])\n",
        "                    last_aa_model = res.get_id()[1]\n",
        "            a = 0\n",
        "            for a in range(len(discont) - 1):\n",
        "                if discont[a] < posmf:\n",
        "                    if discont[a + 1] != discont[a] + 1:\n",
        "                        posmf += 1\n",
        "            # Define the ranges for the superimposer\n",
        "            sup = Superimposer()\n",
        "            ata_q1 = range(posq, posq + 40)\n",
        "            ata_q2 = range(posqf, posqf + 40)\n",
        "            ata_m1 = range(posm, posm + 40)\n",
        "            ata_m2 = range(posmf, posmf + 40)\n",
        "            # Create the list of the residues to be superimposed\n",
        "            ref_atoms = []\n",
        "            q_atoms = []\n",
        "            # Add to the created lists the residues identified before\n",
        "            try:\n",
        "                for ref_chain in chainmodel[0]:\n",
        "                    for ref_res in ref_chain:\n",
        "                        if ref_res.get_id()[1] in ata_m1 or ref_res.get_id()[1] in ata_m2:\n",
        "                            ref_atoms.append(ref_res['CA'])\n",
        "                for q_chain in chainquery[0]:\n",
        "                    for q_res in q_chain:\n",
        "                        if q_res.get_id()[1] in ata_q1 or q_res.get_id()[1] in ata_q2:\n",
        "                            q_atoms.append(q_res['CA'])\n",
        "                    # Use the created lists to execute superimposer if they have the same number of atoms\n",
        "                if len(ref_atoms) == len(q_atoms):\n",
        "                    sup.set_atoms(ref_atoms, q_atoms)\n",
        "                    sup.apply(chainquery)\n",
        "                    io = PDBIO()\n",
        "                    io.set_structure(chainquery)\n",
        "                    # Refine the model until rms>5 or 20 iterations\n",
        "                    print(\"\\nRefining the model for chain \" + str(toalign[k_chain]) + \"...\")\n",
        "                    if numpy.abs(sup.rms) > 7.5:\n",
        "                        checks[str(k_chain)] = 1\n",
        "                        print(\"Careful, the superposition of the chains has an RMS of >5 (RMS = \" + str(\n",
        "                            (numpy.abs(sup.rms)))[0:4] + \").\\n\\n\")\n",
        "                        io.save(\"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "\n",
        "                    else:\n",
        "                        print(\"Perfect for chain \" + toalign[k_chain] + \".\\n\")\n",
        "                        io.save(\"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "\n",
        "                else:\n",
        "                    print(\"\\nThere is some problem with chain \" + str(\n",
        "                        toalign[k_chain]) + \". Trying an alternate solution...\")\n",
        "\n",
        "                    # Identify the most complete chain\n",
        "                    chain_lengths = []\n",
        "                    for chain in ordered_structure.get_chains():\n",
        "                        chain_lengths.append(len(chain))\n",
        "                    complete_chain = toalign[chain_lengths.index(max(chain_lengths))]\n",
        "                    good_model = parser.get_structure(\"CH0\" + complete_chain, \"chain\" + complete_chain + \".pdb\")\n",
        "                    try:\n",
        "                        # Move a copy of the most complete chain to the position of the failed chain\n",
        "                        range_model = range(int(last_aa_model / 2), int((last_aa_model / 2) + 20))\n",
        "                        fchain_atoms = []\n",
        "                        goodm_atoms = []\n",
        "                        for ref_chain in chainmodel[0]:\n",
        "                            for ref_res in ref_chain:\n",
        "                                if ref_res.get_id()[1] in range_model:\n",
        "                                    fchain_atoms.append(ref_res['CA'])\n",
        "                        for goodm_chain in good_model[0]:\n",
        "                            for goodm_res in goodm_chain:\n",
        "                                if goodm_res.get_id()[1] in range_model:\n",
        "                                    goodm_atoms.append(goodm_res['CA'])\n",
        "                        sup.set_atoms(fchain_atoms, goodm_atoms)\n",
        "                        sup.apply(good_model)\n",
        "                        io = PDBIO()\n",
        "                        io.set_structure(good_model)\n",
        "                        io.save(\"good_model\" + toalign[k_chain] + \".pdb\")\n",
        "                        # redefine the chainmodel to use the new superimposed file\n",
        "                        chainmodel_fixed = parser.get_structure(\"CH0M\", \"good_model\" + toalign[k_chain] + \".pdb\")\n",
        "                        ref_atomsfixed = []\n",
        "                        q_atoms = []\n",
        "                        for m_chain in chainmodel_fixed[0]:\n",
        "                            for m_res in m_chain:\n",
        "                                if m_res.get_id()[1] in range_model:\n",
        "                                    ref_atomsfixed.append(m_res['CA'])\n",
        "                        for q_chain in chainquery[0]:\n",
        "                            for q_res in q_chain:\n",
        "                                if q_res.get_id()[1] in range_model:\n",
        "                                    q_atoms.append(q_res['CA'])\n",
        "                        # Superimpose query with the new structure\n",
        "                        sup.set_atoms(ref_atomsfixed, q_atoms)\n",
        "                        sup.apply(chainquery)\n",
        "                        io.set_structure(chainquery)\n",
        "                        print(\"Superimposing the query to a fixed chain \" + str(toalign[k_chain]) + \"...\")\n",
        "                        io.save(\"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "\n",
        "                        count = 0\n",
        "                        if numpy.abs(sup.rms) > 7.5:\n",
        "                            checks[str(k_chain)] = 1\n",
        "                            print(\"Careful, the superposition of the chains has an RMS of >7.5 (RMS = \" + str(\n",
        "                                (numpy.abs(sup.rms)))[0:4] + \").\\n\")\n",
        "\n",
        "                        else:\n",
        "                            print(\"Perfect for chain \" + toalign[k_chain] + \".\")\n",
        "\n",
        "                    except BaseException as error:\n",
        "                        checks[str(k_chain)] = 2\n",
        "                        print(\"Didn't work either. Check the PDB file for discontinuties or errors.\", error)\n",
        "            except KeyError:\n",
        "                checks[str(k_chain)] = 2\n",
        "                print(\"Superimposition for chain \" + str(toalign[k_chain]) + \" did not work. \"\n",
        "                      \"It could be that it's different from the initally modelled monomer.\\n\")\n",
        "                break\n",
        "        return checks\n",
        "\n",
        "    # Check if the model protein is formed by  more than one chain. Although not prove of it being multimeric,\n",
        "    # better more work than overwatch this. If so, ask user if a multimeric model using the template has to be created#\n",
        "    print('Creating your model with Modeller...')\n",
        "    env = Environ()\n",
        "    pdbl.retrieve_pdb_file(pdb_id, pdir='.', file_format='pdb')\n",
        "    model_pdbfile = 'pdb' + pdb_id.casefold() + '.ent'\n",
        "    model_file_name = str(pdb_id)\n",
        "    pdb_model_structure = parser.get_structure(model_file_name, model_pdbfile)\n",
        "    model_chain = model_structure.get_chains() \n",
        "      \n",
        "    chainid = []\n",
        "    for chains in pdb_model_structure.get_chains():\n",
        "        if chains.get_id() not in chainid:\n",
        "            chainid.append(chains.get_id())\n",
        "\n",
        "    number_of_chains = len(chainid)\n",
        "\n",
        "    if number_of_chains > 1:\n",
        "        for i in range(number_of_chains):\n",
        "            query_id =  \"query_aligned\" + str(i) + \".pdb\"\n",
        "            shutil.copyfile(\"query.B99990001.pdb\", query_id)\n",
        "            mdl = Model(env, file=query_id)\n",
        "            mdl.rename_segments(segment_ids=[chainid[i]])\n",
        "            mdl.write(file=query_id)\n",
        "\n",
        "        # split the chains avoiding any error if atoms disordered\n",
        "        # - bug in biopython (https://www.biostars.org/p/380566/ and https://github.com/biopython/biopython/issues/455),\n",
        "        # changed the following def in the PDB python file of biopython. This converst disordered residues into ordered\n",
        "        # ones by eliminating all locations apart from the first one.#\n",
        "        #############################################################################################\n",
        "        # def get_unpacked_list(self):                                                               #\n",
        "        #     \"\"\"                                                                                   #\n",
        "        #     Returns all atoms from the residue,                                                   #\n",
        "        #     in case of disordered, keep only first alt loc and remove the alt-loc tag             #\n",
        "        #     \"\"\"                                                                                   #\n",
        "        #     atom_list = self.get_list()                                                           #\n",
        "        #     undisordered_atom_list = []                                                           #\n",
        "        #     for atom in atom_list:                                                                #\n",
        "        #         if atom.is_disordered():                                                          #\n",
        "        #         if atom.is_disordered():                                                          #\n",
        "        #             atom.altloc=\" \"                                                               #\n",
        "        #             undisordered_atom_list.append(atom)                                           #\n",
        "        #         else:                                                                             #\n",
        "        #             undisordered_atom_list.append(atom)                                           #\n",
        "        #     return undisordered_atom_list                                                         #\n",
        "        #############################################################################################\n",
        "        io = PDBIO()\n",
        "        io.set_structure(pdb_model_structure)\n",
        "        io.save(\"ord_pdb\" + pdb_id.casefold() + \".ent\")\n",
        "        ordered_pdbfile = \"ord_pdb\" + pdb_id.casefold() + \".ent\"\n",
        "        file = pdb_id.casefold()\n",
        "        ordered_structure = parser.get_structure(file, ordered_pdbfile)\n",
        "        k = 1\n",
        "        for chain in ordered_structure.get_chains():\n",
        "            for k in range(number_of_chains):\n",
        "                chain = ordered_structure[0][chainid[k]]\n",
        "                io.set_structure(chain)\n",
        "                io.save(\"chain\" + str(chainid[k]) + \".pdb\")\n",
        "                k += 1\n",
        "\n",
        "        # Use the method defined to superimpose the chains.\n",
        "        toalign = {}\n",
        "        ch = 0\n",
        "        for chains in chainid:\n",
        "            if ch < number_of_chains:\n",
        "                toalign[ch] = chainid[ch]\n",
        "                ch += 1\n",
        "        checks = superimpose_chains(toalign)\n",
        "        \n",
        "        # Add the files together to create a query_template with all subunits\n",
        "        filenames = glob.glob(\"query_aligned*.pdb\")\n",
        "        if 2 not in checks.values():\n",
        "            with open('query_multimeric.pdb', 'w+') as fullquerymodel:\n",
        "                for names in filenames:\n",
        "                    with open(names) as infile:\n",
        "                        for line in infile:\n",
        "                            if 'END' not in line:\n",
        "                                fullquerymodel.write(line)\n",
        "                fullquerymodel.write(\"END\\n\")\n",
        "            if 1 in checks.values():\n",
        "                print(\"While creating your multimeric model some errors occured. You can find your model in \" + str(\n",
        "                    os.getcwd()) + \" in a file named query_multimeric.pdb. \"\n",
        "                                  \"BUT CHECK IT BEFORE USING IT IN ANY FURTHER STEP!\")\n",
        "            else:\n",
        "                print(\n",
        "                    \"\\tFinished running the module Blast&Modeller module!\\nCreation of your model was succesfull. \"\n",
        "                    \"You can find it in \" + str(os.getcwd()) + \" in a file named query_multimeric.pdb along with the \"\n",
        "                    \"monomeric modelled structure (query_monomeric.pdb), the sequences in fasta format, the \"\n",
        "                    \"blast result and the alignment. You can continue now running the ActiveSiteID!\")\n",
        "    else:\n",
        "        mdl = Model(env, file=\"query.B99990001.pdb\")\n",
        "        mdl.rename_segments(segment_ids=\"A\")\n",
        "        mdl.write(file=\"query.B99990001.pdb\")\n",
        "        print(\"\\tFinished running the module Blast&Modeller module!\\n You can find your result files in \" + str(\n",
        "            working_directory) + \" in the Blast&Modeller folder.\\nIf you think your protein should have a multimeric \" +\n",
        "            \"assembly, check https://www.rcsb.org/structure/\" + str(pdb_id) +\n",
        "            \" for more information. You can continue now running the ActiveSiteID!\")\n",
        "\n",
        "    # Clean the directory of intermediate files which are not necessary\n",
        "    try:\n",
        "        os.rmdir(\"obsolete\")\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    os.rename(\"query.B99990001.pdb\", \"query_monomer.pdb\")\n",
        "    if os.path.isfile('salign1.ali') == True:\n",
        "        os.rename(\"salign1.ali\", \"alignment.ali\")\n",
        "    os.rename(model_pdbfile, 'pdb_template.pdb')\n",
        "    os.rename('./' +identifier +'.pdb', 'alphafold_template.pdb')\n",
        "    keep = [\"query_multimeric.pdb\", \"query_monomer.pdb\", \"blast_result.xml\", 'uniprot_blast_result.xml',\n",
        "            'pdb_template.pdb', \"query.fasta\", \"model.fasta\", ]\n",
        "    files = []\n",
        "    for f in os.listdir(\".\"):\n",
        "        if os.path.isfile:\n",
        "            files.append(f)\n",
        "\n",
        "    for f in files:\n",
        "        if f not in keep:\n",
        "            try:\n",
        "              os.remove(f)\n",
        "            except BaseException:\n",
        "              pass\n",
        "\n",
        "    os.chdir(initial_location)  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main(PROJECT_NAME, PROTEIN_SEQUENCE, MODELLING)"
      ],
      "metadata": {
        "id": "OOdZqJ2DqjX2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown View generated model (red) and the template (blue).\n",
        "\n",
        "#@markdown Indicate the Modeller folder where the files are contained. Ex. /content/Test/Modeller\n",
        "MODELLER_FOLDER = \"/content/TEST/Modeller\" #@param {type:\"string\"}\n",
        "\n",
        "os.chdir(MODELLER_FOLDER)\n",
        "os.getcwd()\n",
        "\n",
        "import py3Dmol\n",
        "view = py3Dmol.view()\n",
        "view.addModel(open('pdb_template.pdb','r').read(),'pdb')\n",
        "try:\n",
        "    view.addModel(open('query_multimeric.pdb', 'r').read(),'pdb')\n",
        "except FileNotFoundError:\n",
        "    view.addModel(open('query_monomeric.pdb', 'r').read(),'pdb')\n",
        "\n",
        "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'red'}})\n",
        "view.setStyle({'model': 0}, {\"cartoon\": {'color': 'blue'}})\n",
        "view.zoomTo()\n",
        "view.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "08CiyTawffLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODULE 1.1 - QUATERNARY STRUCTURE\n",
        "#@markdown Quaternary structure mimic from a template pdb file.\n",
        "\n",
        "#@markdown Indicate the PDB id of the protein you want to mimic the quaternary structure to.\n",
        "PDB_FILE = \"4A6T\" #@param {type:\"string\"}\n",
        "#@markdown Indicate the full path to your monomeric model. Ex.\n",
        "MONOMERIC_STRUCTURE = \"/content/TEST/Modeller/query_monomer.pdb\" #@param {type:\"string\"}\n",
        "\n",
        "warnings.simplefilter('ignore', BiopythonWarning)\n",
        "\n",
        "working_directory = os.path.abspath((os.path.join(MONOMERIC_STRUCTURE, os.pardir)))\n",
        "%cd $working_directory\n",
        "keep = os.listdir('.')\n",
        "\n",
        "def superimpose_chains(dict):\n",
        "    checks = {}\n",
        "    # Check if clustalw is available.\n",
        "    for k_chain in toalign.keys():\n",
        "        chainmodel = parser.get_structure(\"CH0\" + toalign[k_chain], \"chain\" + toalign[k_chain] + \".pdb\")\n",
        "        chainquery = parser.get_structure(\"Q00\" + str(k_chain), \"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "        ppb = PPBuilder()\n",
        "        # To maximise the accuracy of the superimposer and avoid problems with alignments of low quality instead\n",
        "        # of using the whole protein to superimpose, two regions at the start and end will be used. To do so, first\n",
        "        # it's necessary to identify two regions at the start and end where both there are no gaps, checking the\n",
        "        # alignment for a minimum of 40 positions without \"-\" in reading frames of 5 aminoacids.\n",
        "        frac_chainmodelseq = []\n",
        "        chainmodelseq = \"\"\n",
        "        for pp in ppb.build_peptides(chainmodel):\n",
        "            frac_chainmodelseq.append(pp.get_sequence())\n",
        "        for fractions in frac_chainmodelseq:\n",
        "            chainmodelseq += fractions\n",
        "        for pp in ppb.build_peptides(chainquery):\n",
        "            chainqueryseq = pp.get_sequence()\n",
        "        with open(\"initial\" + str(k_chain) + \".fasta\", \"w\") as input_file:\n",
        "            input_file.write(\n",
        "                \">model_\" + toalign[k_chain] + \"\\n\" + str(chainmodelseq) + \"\\n>query_\" + str(k_chain) + \"\\n\" + str(\n",
        "                    chainqueryseq))\n",
        "\n",
        "        ex_clustal = ClustalwCommandline(clustalw_exe, infile=\"initial\" + str(k_chain) + \".fasta\")\n",
        "        stdout, stderr = ex_clustal()\n",
        "        alignment = AlignIO.read(\"initial\" + str(k_chain) + \".aln\", \"clustal\")\n",
        "        length = len(alignment[0])\n",
        "        # Identify the first region of 40 aa without gaps in the alignment\n",
        "        i = 0\n",
        "        posm = 0\n",
        "        posq = 0\n",
        "        posmf = 0\n",
        "        posqf = 0\n",
        "        for a in range(int(length / 5)):\n",
        "            if \"-\" not in alignment[0][i:i + 40].seq and \"-\" not in alignment[1][i:i + 40]:\n",
        "                posq = i\n",
        "                posm = i\n",
        "            else:\n",
        "                i += 5\n",
        "        # Translate the alignment position to the real position in each sequence\n",
        "        for aa in alignment[0].seq[0:i]:\n",
        "            if aa == \"-\":\n",
        "                posm -= 1\n",
        "        for aa in alignment[1].seq[0:i]:\n",
        "            if aa == \"-\":\n",
        "                posq -= 1\n",
        "        # Identify in the last quarter of the the alignment, a region of 40 aa without gaps\n",
        "        k = length - (length // 4)\n",
        "        while k < length - 40:\n",
        "            if \"-\" not in alignment[0][k:k + 40].seq and \"-\" not in alignment[1][k:k + 40].seq:\n",
        "                posqf = k\n",
        "                posmf = k\n",
        "                break\n",
        "            else:\n",
        "                k += 5\n",
        "        if k + 40 > length:\n",
        "            k = length - (length // 4)\n",
        "            while k < length - 40:\n",
        "                if \"-\" not in alignment[0][k:k + 40].seq and \"-\" not in alignment[1][k:k + 40].seq:\n",
        "                    posqf = k\n",
        "                    posmf = k\n",
        "                    break\n",
        "                else:\n",
        "                    k += 2\n",
        "        if posqf == 0:\n",
        "            posqf = length - (length // 4)\n",
        "            posmf = length - (length // 4)\n",
        "        # Translate the alignment position to the real position in each sequence\n",
        "        gapsmodel = 0\n",
        "        for aa in alignment[0].seq[0:k]:\n",
        "            if aa == \"-\":\n",
        "                gapsmodel += 1\n",
        "                posmf -= 1\n",
        "        for aa in alignment[1].seq[0:k]:\n",
        "            if aa == \"-\":\n",
        "                posqf -= 1\n",
        "        # Check if the pdb file starts at amino acid 1 or not. If not, fix the range to be applied to the model.\n",
        "        mdel_res_list = unfold_entities(chainmodel, \"R\")\n",
        "        first_aa_model = mdel_res_list[0].get_id()[1]\n",
        "        if first_aa_model != 1:\n",
        "            posm = posm + first_aa_model\n",
        "            posmf = posmf + first_aa_model\n",
        "\n",
        "        discont = []\n",
        "        for res in mdel_res_list:\n",
        "            if is_aa(res) is True:\n",
        "                discont.append(res.get_id()[1])\n",
        "                last_aa_model = res.get_id()[1]\n",
        "        a = 0\n",
        "        for a in range(len(discont) - 1):\n",
        "            if discont[a] < posmf:\n",
        "                if discont[a + 1] != discont[a] + 1:\n",
        "                    posmf += 1\n",
        "        # Define the ranges for the superimposer\n",
        "        sup = Superimposer()\n",
        "        ata_q1 = range(posq, posq + 40)\n",
        "        ata_q2 = range(posqf, posqf + 40)\n",
        "        ata_m1 = range(posm, posm + 40)\n",
        "        ata_m2 = range(posmf, posmf + 40)\n",
        "        # Create the list of the residues to be superimposed\n",
        "        ref_atoms = []\n",
        "        q_atoms = []\n",
        "        # Add to the created lists the residues identified before\n",
        "        try:\n",
        "            for ref_chain in chainmodel[0]:\n",
        "                for ref_res in ref_chain:\n",
        "                    if ref_res.get_id()[1] in ata_m1 or ref_res.get_id()[1] in ata_m2:\n",
        "                        ref_atoms.append(ref_res['CA'])\n",
        "            for q_chain in chainquery[0]:\n",
        "                for q_res in q_chain:\n",
        "                    if q_res.get_id()[1] in ata_q1 or q_res.get_id()[1] in ata_q2:\n",
        "                        q_atoms.append(q_res['CA'])\n",
        "                # Use the created lists to execute superimposer if they have the same number of atoms\n",
        "            if len(ref_atoms) == len(q_atoms):\n",
        "                sup.set_atoms(ref_atoms, q_atoms)\n",
        "                sup.apply(chainquery)\n",
        "                io = PDBIO()\n",
        "                io.set_structure(chainquery)\n",
        "                # Refine the model until rms>5 or 20 iterations\n",
        "                print(\"\\nRefining the model for chain \" + str(toalign[k_chain]) + \"...\")\n",
        "                if numpy.abs(sup.rms) > 7.5:\n",
        "                    checks[str(k_chain)] = 1\n",
        "                    print(\"Careful, the superposition of the chains has an RMS of >5 (RMS = \" + str(\n",
        "                        (numpy.abs(sup.rms)))[0:4] + \").\\n\\n\")\n",
        "                    io.save(\"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "\n",
        "                else:\n",
        "                    print(\"Perfect for chain \" + toalign[k_chain] + \".\\n\")\n",
        "                    io.save(\"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "\n",
        "            else:\n",
        "                print(\"\\nThere is some problem with chain \" + str(\n",
        "                    toalign[k_chain]) + \". Trying an alternate solution...\")\n",
        "\n",
        "                # Identify the most complete chain\n",
        "                chain_lengths = []\n",
        "                for chain in ordered_structure.get_chains():\n",
        "                    chain_lengths.append(len(chain))\n",
        "                complete_chain = toalign[chain_lengths.index(max(chain_lengths))]\n",
        "                good_model = parser.get_structure(\"CH0\" + complete_chain, \"chain\" + complete_chain + \".pdb\")\n",
        "                try:\n",
        "                    # Move a copy of the most complete chain to the position of the failed chain\n",
        "                    range_model = range(int(last_aa_model / 2), int((last_aa_model / 2) + 20))\n",
        "                    fchain_atoms = []\n",
        "                    goodm_atoms = []\n",
        "                    for ref_chain in chainmodel[0]:\n",
        "                        for ref_res in ref_chain:\n",
        "                            if ref_res.get_id()[1] in range_model:\n",
        "                                fchain_atoms.append(ref_res['CA'])\n",
        "                    for goodm_chain in good_model[0]:\n",
        "                        for goodm_res in goodm_chain:\n",
        "                            if goodm_res.get_id()[1] in range_model:\n",
        "                                goodm_atoms.append(goodm_res['CA'])\n",
        "                    sup.set_atoms(fchain_atoms, goodm_atoms)\n",
        "                    sup.apply(good_model)\n",
        "                    io = PDBIO()\n",
        "                    io.set_structure(good_model)\n",
        "                    io.save(\"good_model\" + toalign[k_chain] + \".pdb\")\n",
        "                    # redefine the chainmodel to use the new superimposed file\n",
        "                    chainmodel_fixed = parser.get_structure(\"CH0M\", \"good_model\" + toalign[k_chain] + \".pdb\")\n",
        "                    ref_atomsfixed = []\n",
        "                    q_atoms = []\n",
        "                    for m_chain in chainmodel_fixed[0]:\n",
        "                        for m_res in m_chain:\n",
        "                            if m_res.get_id()[1] in range_model:\n",
        "                                ref_atomsfixed.append(m_res['CA'])\n",
        "                    for q_chain in chainquery[0]:\n",
        "                        for q_res in q_chain:\n",
        "                            if q_res.get_id()[1] in range_model:\n",
        "                                q_atoms.append(q_res['CA'])\n",
        "                    # Superimpose query with the new structure\n",
        "                    sup.set_atoms(ref_atomsfixed, q_atoms)\n",
        "                    sup.apply(chainquery)\n",
        "                    io.set_structure(chainquery)\n",
        "                    print(\"Superimposing the query to a fixed chain \" + str(toalign[k_chain]) + \"...\")\n",
        "                    io.save(\"query_aligned\" + str(k_chain) + \".pdb\")\n",
        "\n",
        "                    count = 0\n",
        "                    if numpy.abs(sup.rms) > 7.5:\n",
        "                        checks[str(k_chain)] = 1\n",
        "                        print(\"Careful, the superposition of the chains has an RMS of >7.5 (RMS = \" + str(\n",
        "                            (numpy.abs(sup.rms)))[0:4] + \").\\n\")\n",
        "\n",
        "                    else:\n",
        "                        print(\"Perfect for chain \" + toalign[k_chain] + \".\")\n",
        "\n",
        "                except BaseException as error:\n",
        "                    checks[str(k_chain)] = 2\n",
        "                    print(\"Didn't work either. Check the PDB file for discontinuties or errors.\", error)\n",
        "        except KeyError:\n",
        "            checks[str(k_chain)] = 2\n",
        "            print(\"Superimposition for chain \" + str(toalign[k_chain]) + \" did not work. \"\n",
        "                  \"It could be that it's different from the initally modelled monomer.\\n\")\n",
        "            break\n",
        "    return checks\n",
        "\n",
        "try:\n",
        "    shutil.copy(MONOMERIC_STRUCTURE, \"./monomeric_structure.pdb\")\n",
        "except OSError:\n",
        "    print(\"Monomeric model not found. Please try to answer again.\")\n",
        "answer_pdb_id = PDB_FILE\n",
        "if len(answer_pdb_id) == 4:\n",
        "    if answer_pdb_id.isalpha() is True:\n",
        "        print(\"It seems that your format is not a PDBid. Try again.\")\n",
        "    elif answer_pdb_id.isnumeric() is True:\n",
        "        print(\"Remember that PDBid are formed by letters and numbers.\")\n",
        "    else:\n",
        "        id_1 = answer_pdb_id\n",
        "        print(str(id_1) + \" is going to be used as the template for your model.\")\n",
        "else:\n",
        "    print(\"That doesn't seem to be an accepted format.\")\n",
        "\n",
        "urllib.request.urlretrieve(\"https://files.rcsb.org/download/\" + id_1 + \".pdb1\", './' + id_1 + '.pdb')\n",
        "parser = PDBParser()\n",
        "ppb = PPBuilder()\n",
        "pdbfile = id_1 + '.pdb'\n",
        "file_name = str(id_1.casefold())\n",
        "structure = parser.get_structure(PDB_FILE, pdbfile)\n",
        "env = Environ()\n",
        "\n",
        "chainid = []\n",
        "for chains in structure.get_chains():\n",
        "    if chains.get_id() not in chainid:\n",
        "        chainid.append(chains.get_id())\n",
        "\n",
        "number_of_chains = len(chainid)\n",
        "\n",
        "if number_of_chains > 1:\n",
        "    i = 0\n",
        "    for i in range(number_of_chains):\n",
        "        shutil.copyfile(\"monomeric_structure.pdb\", \"query_aligned\" + str(i) + \".pdb\")\n",
        "    # Give the newly created files a chain id.#\n",
        "    chainid = []\n",
        "    for chains in structure.get_chains():\n",
        "        chainid.append(chains.get_id())\n",
        "    chainrange = range(0, len(chainid))\n",
        "    filenames = []\n",
        "    for files in glob.glob(\"query_aligned*.pdb\", recursive=True):\n",
        "        filenames.append(files)\n",
        "    i = 0\n",
        "    for files in filenames:\n",
        "        while i < len(filenames):\n",
        "            mdl = Model(env, file=filenames[i])\n",
        "            mdl.rename_segments(segment_ids=[chainid[i]])\n",
        "            mdl.write(file=\"query_aligned\"+str(i)+\".pdb\")\n",
        "            i += 1\n",
        "\n",
        "io = PDBIO()\n",
        "io.set_structure(structure)\n",
        "io.save(\"ord_pdb\" + id_1.casefold() + \".ent\")\n",
        "ordered_pdbfile = \"ord_pdb\" + id_1.casefold() + \".ent\"\n",
        "file = id_1.casefold()\n",
        "ordered_structure = parser.get_structure(file, ordered_pdbfile)\n",
        "k = 0\n",
        "for chain in ordered_structure.get_chains():\n",
        "    for k in range(number_of_chains):\n",
        "        chain = ordered_structure[0][chainid[k]]\n",
        "        io.set_structure(chain)\n",
        "        io.save(\"chain\" + str(chainid[k]) + \".pdb\")\n",
        "        k += 1\n",
        "\n",
        "# Use the method defined to superimpose the chains.\n",
        "toalign = {}\n",
        "ch = 0\n",
        "for chains in chainid:\n",
        "    if ch < number_of_chains:\n",
        "        toalign[ch] = chainid[ch]\n",
        "        ch += 1\n",
        "        \n",
        "checks = superimpose_chains(toalign)\n",
        "filenames = glob.glob(\"query_aligned*.pdb\")\n",
        "if 2 not in checks.values():\n",
        "    if os.path.isfile(\"query_multimeric\") == True:\n",
        "        with open('quat_multimeric.pdb', 'w+') as fullquerymodel:  \n",
        "            for names in filenames: \n",
        "                with open(names) as infile: \n",
        "                    for line in infile:\n",
        "                        if 'END' not in line:\n",
        "                            fullquerymodel.write(line)\n",
        "            fullquerymodel.write(\"END\\n\")\n",
        "    else:\n",
        "        with open('quat_as_' + answer_pdb_id + '.pdb', 'w+') as fullquerymodel:  \n",
        "            for names in filenames: \n",
        "                with open(names) as infile: \n",
        "                    for line in infile:\n",
        "                        if 'END' not in line:\n",
        "                            fullquerymodel.write(line)\n",
        "            fullquerymodel.write(\"END\\n\")\n",
        "    if 1 in checks.values():\n",
        "        print(\"While creating your multimeric model some errors occured. You can find your model in \"\n",
        "              + str(os.getcwd()) + \" in a file named query_multimeric.pdb. BUT CHECK IT BEFORE USING IT IN ANY \"\n",
        "                                    \"FURTHER STEP!\")\n",
        "    else:\n",
        "        print(\"\\tFinished running the module Blast&Modeller module!\\nCreation of your model was succesfull.\"\n",
        "              \"You can find it in \" + str(os.getcwd()) + \" in a file named query_multimeric.pdb along with \"\n",
        "              \"the monomeric modelled structure (query_monomeric.pdb), the sequences in fasta format, \"\n",
        "              \"the blast result and the alignment. You can continue now running the ActiveSiteID!\")\n",
        "else:\n",
        "    print(\"No multimeric assembly could be modelled due to errors in the chain or too high RMS. \"\n",
        "          \"Please check the pdb file for discontinuities or related problems \"\n",
        "          \"(check https://www.rcsb.org/structure/\" + str(id_1) + \"for more information). \"\n",
        "          \"You could try to rerun the stand alone multimeric builder with another PDB id.\")\n",
        "   \n",
        "keep.append(id_1 + '.pdb')\n",
        "keep.append('quat_as_' + answer_pdb_id + '.pdb')\n",
        "\n",
        "files = []\n",
        "for f in os.listdir(\".\"):\n",
        "    if os.path.isfile:\n",
        "        files.append(f)\n",
        "\n",
        "for f in files:\n",
        "    if f not in keep:\n",
        "        os.remove(f)"
      ],
      "metadata": {
        "id": "Dk41TLM9Lzxh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown View generated model (red) and the template (blue).\n",
        "\n",
        "view = py3Dmol.view()\n",
        "files = glob.glob('quat_as_*')\n",
        "for pdb in files:\n",
        "  original_pdb = pdb.split('_')[2]\n",
        "\n",
        "view.addModel(open(original_pdb,'r').read(),'pdb')\n",
        "\n",
        "view.addModel(open(files[0], 'r').read(),'pdb')\n",
        "\n",
        "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'red'}})\n",
        "view.setStyle({'model': 0}, {\"cartoon\": {'color': 'blue'}})\n",
        "view.zoomTo()"
      ],
      "metadata": {
        "id": "OAjMeADWfVLZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODULE 2 - ACTIVE SITE ID\n",
        "#@markdown Identify the active site of your protein by comparing it to the MS-CSA (https://www.ebi.ac.uk/thornton-srv/m-csa/)\n",
        "\n",
        "#@markdown Indicate if you have run before the MODELLER module in the same project.\n",
        "MODELLER_RUN = True #@param {type:\"boolean\"}\n",
        "#@markdown Indicate the full path to your query protein sequence in fasta format. This can be in the Modeller directory (Ex. /content/test/query.fasta )\n",
        "SEQUENCE_FASTA = \"/content/TEST/Modeller/query.fasta\" #@param {type:\"string\"}\n",
        "#@markdown Choose if you want to directly view the highlited position of your active site.\n",
        "SHOW_STRUCTURE = True #@param {type:\"boolean\"}\n",
        "\n",
        "if MODELLER_RUN == True:\n",
        "  modeller_directory = os.path.abspath((os.path.join(SEQUENCE_FASTA, os.pardir)))\n",
        "  working_directory = os.path.abspath((os.path.join(modeller_directory, os.pardir)))\n",
        "else:\n",
        "  working_directory = os.getcwd()\n",
        "\n",
        "%cd $working_directory\n",
        "if os.path.exists(\"ActiveSite\") is False:\n",
        "    os.mkdir(\"ActiveSite\")\n",
        "\n",
        "os.chdir(\"./ActiveSite\")\n",
        "\n",
        "url_atlas = \"https://www.ebi.ac.uk/thornton-srv/m-csa/media/flat_files/curated_data.csv\"\n",
        "print(\"Downloading the curated data from the Mechanism and Catalytic Site atlas - www.ebi.ac.uk -.\")\n",
        "with urllib.request.urlopen(url_atlas) as data, open('./curated_data.csv', 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(data.read().decode())\n",
        "\n",
        "try:\n",
        "  shutil.copy(SEQUENCE_FASTA, \"./query.fasta\")\n",
        "except FileNotFoundError:\n",
        "  print('Make sure your SEQUENCE_FASTA is the full path to your file!')\n",
        "  quit()\n",
        "\n",
        "if MODELLER_RUN == True:\n",
        "    with open(modeller_directory + \"/model.fasta\") as file:\n",
        "        id_1 = file.readline().replace(\">\", \"\").rstrip().upper()\n",
        "elif MODELLER_RUN == False:\n",
        "    query = SeqIO.read(SEQUENCE_FASTA, \"fasta\")\n",
        "    pdb_blast = NCBIWWW.qblast(\"blastp\",\n",
        "                               \"pdbaa\",\n",
        "                                query,\n",
        "                                auto_format='XML')\n",
        "    with open('pdb_blast_result.xml', 'w') as out_handle:\n",
        "        out_handle.write(pdb_blast.read())\n",
        "    \n",
        "    pdb_blast.close()   \n",
        "    blast_pdb_result = SearchIO.read(\"pdb_blast_result.xml\", \"blast-xml\") \n",
        "    if len(blast_pdb_result) > 0:\n",
        "        blast_qresult = SearchIO.read(\"pdb_blast_result.xml\", \"blast-xml\")[0]\n",
        "        best_hit = blast_qresult[0]\n",
        "        id_1 = best_hit.hit.id.split(\"|\")[1]\n",
        "    else:\n",
        "        print('There seems to be no good homologue in the PDB... Sorry!')\n",
        "        quit()\n",
        "\n",
        "print(\"Searching for the uniprot equivalent of your pdb file...\")\n",
        "url = 'https://rest.uniprot.org/idmapping/run'\n",
        "params = {'from': 'PDB', 'to': 'UniProtKB', 'ids': id_1}\n",
        "headers = {'user_agent': 'Mozilla/5.0'}\n",
        "\n",
        "try:       \n",
        "    pdb_id = id_1\n",
        "    handle = requests.get(\n",
        "        'https://www.ebi.ac.uk/pdbe/api/mappings/' + pdb_id)\n",
        "    handle_json = handle.json()\n",
        "    for key, item in handle_json[pdb_id.lower()].items():\n",
        "        if key == 'UniProt':\n",
        "            uniprotID = list(item.keys())[0]\n",
        "            print('Found the uniprot equivalency: ' + uniprotID)\n",
        "except BaseException:\n",
        "    Entrez.email = 'davidrourap@gmail.com'\n",
        "    handle_random_id = Entrez.esearch(\n",
        "        db=\"protein\", term=id_1)\n",
        "    record_random_id = Entrez.read(handle_random_id)\n",
        "    entrez_id = record_random_id['IdList'][0]\n",
        "\n",
        "    with Entrez.efetch(db=\"protein\", rettype=\"gb\", retmode=\"text\", id=entrez_id) as handle:\n",
        "        seq_record = SeqIO.read(handle, \"gb\")\n",
        "\n",
        "    GB_id = seq_record.id\n",
        "    fullURL = ('http://rest.uniprot.org/uniprot/search?'\n",
        "                'query=xref:' + str(GB_id) + '&format=list')\n",
        "    result = requests.get(fullURL)\n",
        "    uniprotID = str(result.text).replace('\\n', '')\n",
        "    print('Found the uniprot equivalency: ' + uniprotID)\n",
        "if uniprotID.isalpha():\n",
        "    print(\"Your pdb does not appear to have a UniProt equivalent. Sorry!\")\n",
        "else:\n",
        "    print(\"Uniprot id found. Extracting information...\")\n",
        "\n",
        "handle = urllib.request.urlopen(\"https://www.uniprot.org/uniprot/\"+str(uniprotID)+\".xml\")\n",
        "record = SeqIO.read(handle, \"uniprot-xml\")\n",
        "\n",
        "protein_type = \"\"\n",
        "proteinECnumber = \"\"\n",
        "\n",
        "for information in record:\n",
        "    if \"submittedName_fullName\" in record.annotations:\n",
        "        protein_type = record.annotations[\"submittedName_fullName\"]\n",
        "        break\n",
        "    elif \"recommendedName_fullName\" in record.annotations:\n",
        "        protein_type = str(record.annotations[\"recommendedName_fullName\"])\n",
        "        break\n",
        "\n",
        "for information in record:\n",
        "    if \"submittedName_ecNumber\" in record.annotations:\n",
        "        proteinECnumber = record.annotations['submittedName_ecNumber']\n",
        "        break\n",
        "    elif \"recommendedName_ecNumber\" in record.annotations:\n",
        "        proteinECnumber = record.annotations['recommendedName_ecNumber']\n",
        "        break\n",
        "\n",
        "if isinstance(proteinECnumber, list):\n",
        "    proteinECnumber = str(proteinECnumber[0])\n",
        "\n",
        "if proteinECnumber == \"\":\n",
        "    print(\"It seems the information provided by uniprot is not enough. \"\n",
        "          \"Please, check https://www.uniprot.org/uniprot/\"+str(uniprotID))\n",
        "    print(\"Your protein doesn't seem to have an EC number... Not much we can do without it!\")\n",
        "    if proteinECnumber.upper().replace(\" \", \"\") == \"NO\":\n",
        "        print(\"Sorry, but without the EC number there is little we can do...\")\n",
        "        time.sleep(3)\n",
        "        quit()\n",
        "\n",
        "\n",
        "res = open(\"Active_site.txt\", \"a\")\n",
        "if isinstance(protein_type, list) is True:\n",
        "    print(\"\\nYour model protein, \" + str(id_1)+\", has been identified as a \" + protein_type[0] +\n",
        "          \" with EC number \" + str(proteinECnumber) + \".\\n\")\n",
        "    print(\"\\nYour model protein, \" + str(id_1)+\", has been identified as a \" + protein_type[0] +\n",
        "          \" with EC number \" + str(proteinECnumber) + \".\\n\", file=res)\n",
        "elif isinstance(protein_type, str) is True:\n",
        "    print(\"\\nYour model protein, \" + str(id_1)+\", has been identified as a \"\n",
        "          + protein_type.replace(\"[\", \"\").replace(\"]\", \"\") + \" with EC number \" + str(proteinECnumber) + \".\\n\")\n",
        "    print(\"\\nYour model protein, \" + str(id_1)+\", has been identified as a \"\n",
        "          + protein_type.replace(\"[\", \"\").replace(\"]\", \"\") + \" with EC number \" + str(proteinECnumber) + \".\\n\",\n",
        "          file=res)\n",
        "else:\n",
        "    print(\"\\nYour model protein, \" + str(id_1)+\", has EC number \" + str(proteinECnumber) + \".\\n\")\n",
        "    print(\"\\nYour model protein, \" + str(id_1)+\", has EC number \" + str(proteinECnumber) + \".\\n\", file=res)\n",
        "res.close()\n",
        "time.sleep(3)\n",
        "\n",
        "# In a few cases, uniprot provides already the information about the catalytic site. In this case, we will compare\n",
        "# the pdb file provided to the query and avoid using the ATLAS site.\n",
        "act_site = []\n",
        "bind_site = []\n",
        "catalytic_res = {}\n",
        "cofactor = []\n",
        "\n",
        "# Find if information about the catalytic residues exists in the uniprot xml\n",
        "for i in range(len(record.features)):\n",
        "    if record.features[i].type == \"active site\":\n",
        "        act_site.append(int(record.features[i].location.end)-1)\n",
        "    elif record.features[i].type == \"binding site\":\n",
        "        bind_site.append(int(record.features[i].location.end)-1)\n",
        "\n",
        "cofactorslist = [\"NAD\", \"FAD\", \"FMN\", \"SAM\", \"PLP\", \"ATP\", \"UTP\", \"ADP\", \"UDP\", \"CTP\", \"PAPS\", \"acetyl COA\", \"Zn\",\n",
        "                 \"Fe\", \"Cu\", \"K\", \"Mg\", \"Mo\", \"Ni\", \"Se\"]\n",
        "if \"comment_cofactor\" in record.annotations:\n",
        "    for ncof in cofactorslist:\n",
        "        for act in range(len(record.annotations[\"comment_cofactor\"])):\n",
        "            if ncof in record.annotations[\"comment_cofactor\"][act]:\n",
        "                cofactor.append(ncof)\n",
        "\n",
        "# If it exists, extract that information and ask for user confirmation to use it\n",
        "if len(act_site) > 0 and use_uniprot == 'YES':\n",
        "    up_maxid = record.annotations[\"accessions\"][0]\n",
        "    for i in range(len(act_site)):\n",
        "        catalytic_res[record.seq[act_site[i]]] = act_site[i]\n",
        "    copyfile(\"query.fasta\", \"refsequp.fasta\")\n",
        "    with open(\"refsequp.fasta\", \"a\") as f:\n",
        "        f.write(\"\\n>\" + str(record.annotations[\"accessions\"][0])+\"\\n\" + str(record.seq))\n",
        "\n",
        "if len(act_site) == 0 or use_uniprot == \"NO\":\n",
        "    print(\"Your model protein does not have an active site identified in UniProt. \"\n",
        "          \"Proceeding to check in the M-CSA database...\")\n",
        "    time.sleep(3)\n",
        "    # Create dictionary from database with uniprot id and EC number only of those\n",
        "    # sharing the same ECnumber with the model#\n",
        "    upidEC = {}\n",
        "    curated_data = csv.reader(open(\"./curated_data.csv\", \"r\"))\n",
        "    for row in curated_data:\n",
        "        upidEC[row[3]] = str(row[1])\n",
        "    searchUP_dict = {}\n",
        "    while len(searchUP_dict) == 0:\n",
        "        for key, value in upidEC.items():\n",
        "            if proteinECnumber in key:\n",
        "                searchUP_dict[key] = value\n",
        "        else:\n",
        "            proteinECnumberlist = proteinECnumber.split(\".\")\n",
        "            maxi = len(proteinECnumberlist)\n",
        "            proteinECnumber = str(proteinECnumberlist[0]) + \".\" + str(proteinECnumberlist[1])\n",
        "            if proteinECnumber in key:\n",
        "                searchUP_dict[key] = value\n",
        "\n",
        "    # Clean uniprot ids if they have more than 6 characters\n",
        "    uniprotids = list(searchUP_dict.values())\n",
        "    uniprotids_clean = []\n",
        "    for i in uniprotids:\n",
        "        if len(i) != 6:\n",
        "            uniprotids_clean.append(i[0:6])\n",
        "        else:\n",
        "            uniprotids_clean.append(i)\n",
        "    # Retrieve sequences of those uniprot identifiers from the website\n",
        "    upECrefseq = []\n",
        "    i = 0\n",
        "    for x in uniprotids_clean:\n",
        "        handle2 = urllib.request.urlopen(\"https://www.uniprot.org/uniprot/\" + uniprotids_clean[i] + \".xml\")\n",
        "        record2 = SeqIO.read(handle2, \"uniprot-xml\")\n",
        "        upECrefseq.append(record2.seq)\n",
        "        i += 1\n",
        "    # Use Pairwise alignment to identify the best hit from the M-CSA list\n",
        "    # Create the fasta file for the identified hit sin the M-CSA\n",
        "    i = 0\n",
        "    for seqs in uniprotids_clean:\n",
        "        with open(uniprotids_clean[i]+\".fasta\", \"w\") as f:\n",
        "            f.write(\"\\n>\"+uniprotids_clean[i]+\"\\n\" + str(upECrefseq[i]))\n",
        "        i += 1\n",
        "\n",
        "    # Pairwise alignment execution. The options used are the typical from the BLOSUM62 substitution matrix\n",
        "\n",
        "    aligner = Align.PairwiseAligner()\n",
        "    alphabet = \"PROTEIN\"\n",
        "    aligner.open_gap_score = -10\n",
        "    aligner.extend_gap_score = -0.1\n",
        "    aligner.substitution_matrix = substitution_matrices.load(\"BLOSUM62\")\n",
        "\n",
        "    # Save into a list the scores of each alignment\n",
        "\n",
        "    alignment_scores = []\n",
        "    i = 0\n",
        "    for seqs in uniprotids_clean:\n",
        "        seq1 = SeqIO.read(\"query.fasta\", \"fasta\")\n",
        "        seq2 = SeqIO.read(uniprotids_clean[i]+\".fasta\", \"fasta\")\n",
        "        alignments = aligner.align(seq1.seq, seq2.seq)\n",
        "        alignment_scores.append(alignments.score)\n",
        "        i += 1\n",
        "\n",
        "    # Idenfity the UniProt ID with the maximal identity to the query sequence\n",
        "\n",
        "    try:\n",
        "        up_maxid = uniprotids_clean[alignment_scores.index(max(alignment_scores))]\n",
        "        print(\"The best hit for your protein is uniprotID: \" + str(up_maxid) + \". \\n\")\n",
        "        time.sleep(3)\n",
        "    except BaseException:\n",
        "        print(\"Could not find a protein with enough homology with your query. \"\n",
        "              \"You can try again with a different model!\")\n",
        "        time.sleep(4)\n",
        "        quit()\n",
        "    time.sleep(2)\n",
        "\n",
        "    # From the UniProtID with max identity, retrieve the catalytic residues annotated in the M-CSA csv file\n",
        "    curated_data = csv.reader(open(\"./curated_data.csv\", \"r\"))\n",
        "    for row in curated_data:\n",
        "        if str(row[1])[0:6] == str(up_maxid) and row[4] == \"residue\":\n",
        "            if row[5] not in catalytic_res.keys():\n",
        "                catalytic_res[row[5]] = [row[7]]\n",
        "            elif row[5] in catalytic_res.keys() and row[7] not in catalytic_res[row[5]]:\n",
        "                catalytic_res[row[5]].append(row[7])\n",
        "\n",
        "    # From the UniProtID with max identity, retrieve the cofactors (if any)\n",
        "    curated_data = csv.reader(open(\"./curated_data.csv\", \"r\"))\n",
        "    for row in curated_data:\n",
        "        if str(row[1]) == str(up_maxid) and row[4] == \"cofactor\":\n",
        "            cofactor.append(row[8] + \"(\" + row[5] + \")\")\n",
        "    cofactor = set(cofactor)\n",
        "\n",
        "# Alignment of the query sequence with the best hit from UniProt to identify the catalytic residues in the query.\n",
        "# Create a fata file containing the query sequence and the sequence of the best UniProtID\n",
        "    shutil.copyfile(\"query.fasta\", \"refsequp.fasta\")\n",
        "\n",
        "    with open(\"refsequp.fasta\", \"a\") as f:\n",
        "        f.write(\"\\n>\"+up_maxid+\"\\n\" + str(upECrefseq[alignment_scores.index(max(alignment_scores))]))\n",
        "\n",
        "# Use ClustalW for the alignment\n",
        "sequp_align = ClustalwCommandline(clustalw_exe, infile=\"refsequp.fasta\", score=\"percent\")\n",
        "stdout, stderr = sequp_align()\n",
        "with open(\"alignment_output.txt\", \"w+\") as clustalscore:\n",
        "    clustalscore.write(stdout)\n",
        "\n",
        "# From the ClustalW results, check if the identity is sufficient to continue with the active site identification\n",
        "with open(\"alignment_output.txt\") as c:\n",
        "    for line in c:\n",
        "        if \"Sequences (1:2)\" in line:\n",
        "            score = int(''.join(filter(str.isdigit, line)))\n",
        "if len(str(score)) == 3:\n",
        "    score = score - 120\n",
        "elif len(str(score)) == 4:\n",
        "    score = score - 1200\n",
        "elif len(str(score)) == 5:\n",
        "    score = 100\n",
        "\n",
        "print(\"\\t Percentage of identity = \" + str(score) + \"\\n\")\n",
        "time.sleep(3)\n",
        "if 40 > score > 15:\n",
        "    identity = 1\n",
        "elif score <= 15:\n",
        "    identity = 2\n",
        "else:\n",
        "    identity = 0\n",
        "\n",
        "time.sleep(3)\n",
        "if identity == 0:\n",
        "    highhomology = \"YES\"\n",
        "elif identity == 1:\n",
        "    highhomology = \"YES\"\n",
        "else:\n",
        "    highhomology = \"NO\"\n",
        "    print(\"Your query sequence alignment resulted in less than 15% identity. \"\n",
        "          \"Prediction of the active site would not be accurate using it.\\n\")\n",
        "    time.sleep(3)\n",
        "\n",
        "# From alignment, and the catalytic residues identified in the M-CSA hit or the UniProt,\n",
        "# return positions in your protein#\n",
        "res = open(\"Active_site.txt\", \"a\")\n",
        "sequp_alignment = AlignIO.read(\"refsequp.aln\", \"clustal\")\n",
        "\n",
        "if highhomology == \"YES\":\n",
        "    # Check that the list is not empty\n",
        "    aaposition_cat = []\n",
        "    aaposition_cat1 = []\n",
        "    if len(list(catalytic_res.keys())[0]) >= 1:\n",
        "        aaname_cat = list(catalytic_res.keys())\n",
        "        aaposition_cat = list(catalytic_res.values())\n",
        "        for aaposition in aaposition_cat:\n",
        "            if isinstance(aaposition, list):\n",
        "                for aa in aaposition:\n",
        "                    aaposition_cat1.append(aa)\n",
        "            else:\n",
        "                aaposition_cat1.append(aaposition)\n",
        "    \n",
        "        # Convert the identified residues to a dictionary with the information in a more readable format\n",
        "        sequp_alignment = AlignIO.read(\"refsequp.aln\", \"clustal\")\n",
        "        threetoone = {\"Ala\": \"A\", \"Arg\": \"R\", \"Asn\": \"N\", \"Asp\": \"D\", \"Cys\": \"C\", \"Glu\": \"E\", \"Gln\": \"Q\",\n",
        "                      \"Gly\": \"G\", \"His\": \"H\", \"Ile\": \"I\", \"Leu\": \"L\", \"Lys\": \"K\", \"Met\": \"M\", \"Phe\": \"F\",\n",
        "                      \"Pro\": \"P\", \"Ser\": \"S\", \"Thr\": \"T\", \"Trp\": \"W\", \"Tyr\": \"Y\", \"Val\": \"V\"}\n",
        "        catalytic_res1lett = {}\n",
        "        i = 0\n",
        "        conv3to1 = 0\n",
        "        for keys in catalytic_res.keys():\n",
        "            if keys in threetoone.keys():\n",
        "                conv3to1 = 1\n",
        "                catalytic_res1lett[threetoone[str(aaname_cat[i])]] = aaposition_cat[i]\n",
        "                i += 1\n",
        "        if conv3to1 == 1:\n",
        "            aaname_cat = list(catalytic_res1lett.keys())\n",
        "        else:\n",
        "            for positions in catalytic_res.keys():\n",
        "                catalytic_res1lett[positions] = [catalytic_res[positions]]  \n",
        "    else:\n",
        "        aaname_cat = list(catalytic_res.keys())\n",
        "        aaposition_cat = list(catalytic_res.values())\n",
        "        aaposition_cat1 = list(catalytic_res.values())\n",
        "        for positions in catalytic_res.keys():\n",
        "            catalytic_res1lett[positions] = [catalytic_res[positions]]\n",
        "    # From the known catalytic positions, identify the corresponding positions in the alignment\n",
        "    print(\"Identifying the predicted catalytic residues in your protein...\\n\")\n",
        "    time.sleep(3)\n",
        "    a = 0\n",
        "    positions_tocheck = []\n",
        "    for it in aaposition_cat1:\n",
        "        positions_tocheck.append(int(it)-1)\n",
        "        a += 1\n",
        "    i = 0\n",
        "    align_catpos = []\n",
        "    for i in range(a):\n",
        "        pos_cat = positions_tocheck[i]\n",
        "        gap_count, gap_count_1, gap_count_2, gap_count_3 = (0, 0, 0, 0)\n",
        "        for aa in sequp_alignment[1][:positions_tocheck[i]]:\n",
        "            if aa == \"-\":\n",
        "                gap_count += 1\n",
        "        if gap_count != 0:\n",
        "            for aa1 in sequp_alignment[1][pos_cat:pos_cat + gap_count]:   \n",
        "                if aa == \"-\":\n",
        "                    gap_count_1 += 1\n",
        "            if gap_count_1 != 0:\n",
        "                for aa2 in sequp_alignment[1][pos_cat:pos_cat + gap_count + gap_count_1]:   \n",
        "                    if aa == \"-\":\n",
        "                        gap_count_2 += 1\n",
        "                if gap_count_2 != 0:\n",
        "                    for aa3 in sequp_alignment[1][pos_cat:pos_cat + gap_count + gap_count_1 + gap_count_2]:   \n",
        "                        if aa == \"-\":\n",
        "                            gap_count_3 += 1\n",
        "        align_catpos.append(pos_cat + gap_count + gap_count_1 + gap_count_2 + gap_count_3)\n",
        "        i += 1\n",
        "    # Extract in list from the dictionary of 1lettcode\n",
        "    lettkeys = list(catalytic_res1lett.keys())\n",
        "\n",
        "    # From the positions in the alignment, identify the real position in the query sequence\n",
        "    query_catalytic_res = {}\n",
        "    for lett in lettkeys:\n",
        "        query_catalytic_res[lett] = []\n",
        "\n",
        "    query = SeqIO.read(\"query.fasta\", \"fasta\")\n",
        "    for pos in align_catpos:\n",
        "        pos_cat = pos\n",
        "        pos_cat_real = pos_cat\n",
        "        for aas in sequp_alignment[0][0:pos_cat+1]:\n",
        "            if aas == \"-\":\n",
        "                pos_cat_real -= 1\n",
        "        try:\n",
        "            if isinstance(query_catalytic_res[query[pos_cat_real]], list) is True:\n",
        "                query_catalytic_res[query[pos_cat_real]].append(pos_cat_real+1)\n",
        "            elif isinstance(query_catalytic_res[query[pos_cat_real]], str) is True:\n",
        "                query_catalytic_res[query[pos_cat_real]] = [query_catalytic_res[query[pos_cat_real]]]\n",
        "                query_catalytic_res[query[pos_cat_real]].append(pos_cat_real+1)\n",
        "        except KeyError:\n",
        "            try:\n",
        "                if isinstance(query_catalytic_res[query[pos_cat_real+1]], list) is True:\n",
        "                    query_catalytic_res[query[pos_cat_real+1]].append(pos_cat_real+2)\n",
        "                elif isinstance(query_catalytic_res[query[pos_cat_real+1]], str) is True:\n",
        "                    query_catalytic_res[query[pos_cat_real+1]] = [query_catalytic_res[query[pos_cat_real+1]]]\n",
        "                    query_catalytic_res[query[pos_cat_real+1]].append(pos_cat_real+2)\n",
        "            except KeyError:\n",
        "                try:\n",
        "                    if isinstance(query_catalytic_res[query[pos_cat_real-1]], list) is True:\n",
        "                        query_catalytic_res[query[pos_cat_real-1]].append(pos_cat_real)\n",
        "                    elif isinstance(query_catalytic_res[query[pos_cat_real-1]], str) is True:\n",
        "                        query_catalytic_res[query[pos_cat_real-1]] = [query_catalytic_res[query[pos_cat_real-1]]]\n",
        "                        query_catalytic_res[query[pos_cat_real-1]].append(pos_cat_real)\n",
        "                except KeyError:\n",
        "                    query_catalytic_res[query[pos_cat_real]] = str(pos_cat_real+1) + \"*\"    \n",
        "\n",
        "    query_catalytic_resC = query_catalytic_res.copy()\n",
        "    for key in query_catalytic_resC.keys():\n",
        "        if len(query_catalytic_resC[key]) == 0:\n",
        "            query_catalytic_res[key] = \"Present in model and absent in the query.\"\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Print the results both in the console and in a file\n",
        "    print(\"The predicted active site is formed by:\")\n",
        "    print(\"The predicted active site is formed by:\", file=res)\n",
        "    for key, value in query_catalytic_res.items():\n",
        "        print(str(key) + \"--> \" + str(value).replace(\"[\", \"\").replace(\"]\", \"\"))\n",
        "        print(str(key) + \"--> \" + str(value).replace(\"[\", \"\").replace(\"]\", \"\"), file=res)\n",
        "    print(\"\\n* : Residue predicted in the query but not present as part of the active site of the model.\\n\")\n",
        "    print(\"\\n* : Residue predicted in the query but not present as part of the active site of the model.\\n\",\n",
        "          file=res)\n",
        "    time.sleep(3)\n",
        "else:\n",
        "    if len(bind_site) > 0:\n",
        "        print(\"Due to low homology, calculation of the active site failed. \"\n",
        "              \"Maybe you could check the model pdb file information in  \"\n",
        "              \"https://www.uniprot.org/uniprot/\"+str(uniprotID) + \" for more information.\")\n",
        "        print(\"Due to low homology, calculation of the active site failed. \"\n",
        "              \"Maybe you could check the model pdb file information in  \"\n",
        "              \"https://www.uniprot.org/uniprot/\"+str(uniprotID) + \" for more information.\", file=res)\n",
        "        time.sleep(3)\n",
        "    else:\n",
        "        print(\"Due to low homology, calculation of the active site failed. \"\n",
        "              \"Maybe you could check the model pdb file information in  \"\n",
        "              \"https://www.uniprot.org/uniprot/\"+str(uniprotID) + \" for more information.\")\n",
        "        print(\"Due to low homology, calculation of the active site failed. \"\n",
        "              \"Maybe you could check the model pdb file information in  \"\n",
        "              \"https://www.uniprot.org/uniprot/\"+str(uniprotID) + \" for more information.\", file=res)\n",
        "\n",
        "# At last, from the EC number write some general information of the protein. If they are supposed to have a cofactor\n",
        "# but could not be identified, print also that!\n",
        "if len(cofactor) > 0:\n",
        "    print(\"\\nIdentified cofactor(s): \")\n",
        "    print(\"\\nIdentified cofactor(s): \", file=res)\n",
        "    for x in cofactor:\n",
        "        print(x)\n",
        "        print(x, file=res)\n",
        "elif len(bind_site) > 0:\n",
        "    print(\"Also, in Uniprot some binding sites are indicated for the pdb model in positions: \")\n",
        "    print(*bind_site, sep=\",\")\n",
        "    print(\"Also, in Uniprot some binding sites are indicated for the pdb model in positions: \", file=res)\n",
        "    print(*bind_site, sep=\",\", file=res)\n",
        "else:\n",
        "    if \"1.\" in proteinECnumber[0:2]:\n",
        "        print(\"Your protein is an oxidoreductase, so it should have a cofactor. But it could not be identified. \"\n",
        "              \"Please, check uniprot entry \" + str(up_maxid) + \" -the best hit identified from the M-CSA database- \"\n",
        "                                                               \"for more information.\")\n",
        "        print(\"Your protein is an oxidoreductase, so it should have a cofactor. But it could not be identified. \"\n",
        "              \"Please, check uniprot entry \" + str(up_maxid) + \" -the best hit identified from the M-CSA database- \"\n",
        "                                                               \"for more information.\", file=res)\n",
        "    elif \"2.\" in proteinECnumber[0:2]:\n",
        "        print(\"Your protein is a transferase, commonly, cofactor dependant. Please, check uniprot entry \"\n",
        "              + str(up_maxid) + \" -the best hit identified- for more information.\")\n",
        "        print(\"Your protein is a transferase, commonly, cofactor dependant. Please, check uniprot entry \"\n",
        "              + str(up_maxid) + \" -the best hit identified- for more information.\", file=res)\n",
        "    elif \"3.\" in proteinECnumber[0:2]:\n",
        "        print(\"Your protein is an hydrolase.\")\n",
        "        print(\"Your protein is an hydrolase.\", file=res)\n",
        "    elif \"4.\" in proteinECnumber[0:2]:\n",
        "        print(\"Your protein is most similar to characterised lyases.\")\n",
        "        print(\"Your protein is most similar to characterised lyases.\", file=res)\n",
        "    elif \"5.\" in proteinECnumber[0:2]:\n",
        "        print(\"Your protein seems to be an isomerase.\")\n",
        "        print(\"Your protein seems to be an isomerase.\", file=res)\n",
        "    elif \"6.\" in proteinECnumber[0:2]:\n",
        "        print(\"Your protein seems to be an ligases. Normally, this enzymes are ATP dependant but no cofactor could \"\n",
        "              \"be identified. Please, check uniprot entry \" + str(up_maxid) +\n",
        "              \" -the best hit identified- for more information.\")\n",
        "        print(\"Your protein seems to be an ligases. Normally, this enzymes are ATP dependant but no cofactor could \"\n",
        "              \"be identified. Please, check uniprot entry \" + str(up_maxid) +\n",
        "              \" -the best hit identified- for more information.\", file=res)\n",
        "\n",
        "res.close()\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "# Clean the directory of intermediate files which are not necessary\n",
        "os.remove(\"refsequp.dnd\")\n",
        "tokeep = [\"query.fasta\", str(up_maxid)+\".fasta\"]\n",
        "for file in glob.glob(\"*.fasta\"):\n",
        "    if file not in tokeep:\n",
        "        os.remove(file)\n",
        "\n",
        "print(\"\\nFinished running the ActiveSiteID!\\n. Your results can be found in \" + str(os.getcwd()) +\n",
        "      \", in a file named Active_site.txt, together with the alignment and the fasta files for your sequences. \"\n",
        "      \"You can run now the Surface&Clusters module! \")\n",
        "\n",
        "if SHOW_STRUCTURE == True:\n",
        "    import py3Dmol\n",
        "    residues = []\n",
        "    with open('Active_site.txt', \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            if \"-->\" in line:\n",
        "                toadd = re.findall(r'\\d+', line)\n",
        "                residues.append(int(toadd[0]))\n",
        "\n",
        "    view = py3Dmol.view()\n",
        "    try:\n",
        "        view.addModel(open('../Modeller/query_multimeric.pdb','r').read(),'pdb')\n",
        "    except FileNotFoundError:\n",
        "        view.addModel(open('../Modeller/query_monomeric.pdb','r').read(),'pdb')\n",
        "    view.setStyle({'model':-1},{'cartoon': {'color':'blue'}})\n",
        "    view.setStyle({'resi': residues}, {\"stick\": {'color': 'red'}})\n",
        "    view.zoomTo()\n",
        "    view.show()"
      ],
      "metadata": {
        "id": "0yPULU2fpuja",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODULE 3 - SURFACE ANALYSIS\n",
        "\n",
        "#@markdown Determine the distance between any user defined residue in your structure and the different clusters.\n",
        "import random\n",
        "\n",
        "#@markdown Indicate the full path to the pdb file you want to analyse. Ex. /content/test/Modeller/query_quaternary.pdb\n",
        "\n",
        "PDB_FILE = \"/content/TEST/Modeller/query_multimeric.pdb\" #@param {type:\"string\"}\n",
        "#@markdown Indicate if you have run before the ACTIVE SITE module in the same project.\n",
        "ACTIVE_SITE = True #@param {type:\"boolean\"}\n",
        "import numpy as np\n",
        "from Bio.PDB import HSExposureCB\n",
        "\n",
        "pdb_directory = os.path.abspath((os.path.join(PDB_FILE, os.pardir)))\n",
        "working_directory = os.path.abspath((os.path.join(pdb_directory, os.pardir)))\n",
        "%cd $working_directory\n",
        "if os.path.exists(\"Surface\") is False:\n",
        "    os.mkdir(\"Surface\")\n",
        "\n",
        "os.chdir(\"./Surface\")\n",
        "\n",
        "try:\n",
        "    shutil.copy(PDB_FILE, './query.pdb')\n",
        "except BaseException:\n",
        "    print('Cannot find the pdb file. Check again the name!')\n",
        "    quit()\n",
        "\n",
        "# Open the desired pdb file\n",
        "parser = PDBParser()\n",
        "fullid = \"Q00F\"\n",
        "fullfile = \"query.pdb\"\n",
        "full_structure = parser.get_structure(fullid, fullfile)\n",
        "mdel = full_structure[0]\n",
        "ppb = PPBuilder()\n",
        "\n",
        "ca_coord = []\n",
        "number_of_chains = 0\n",
        "for chain in mdel.get_list():\n",
        "    number_of_chains += 1\n",
        "for residue in chain.get_list():\n",
        "    if residue.has_id(\"CA\"):\n",
        "        ca = residue[\"CA\"]\n",
        "        ca_coord.append(ca.get_coord())\n",
        "\n",
        "print(\"\\nMeasuring the minimum bounding box...\\n\")\n",
        "\n",
        "ca_coord = np.array(ca_coord)\n",
        "min_x = min(ca_coord[:, 0])\n",
        "min_y = min(ca_coord[:, 1])\n",
        "min_z = min(ca_coord[:, 2])\n",
        "max_x = max(ca_coord[:, 0])\n",
        "max_y = max(ca_coord[:, 1])\n",
        "max_z = max(ca_coord[:, 2])\n",
        "box_dimensions = ([max_x - min_x, max_y - min_y, max_z - min_z])\n",
        "time.sleep(2)\n",
        "volume = box_dimensions[0] * box_dimensions[1] * box_dimensions[2]\n",
        "print(\"Your protein, formed by \" + str(number_of_chains) + \" chains, has an aproximate dimensions of: \"\n",
        "  + str(box_dimensions[0]) + \" A, \" + str(box_dimensions[1]) + \" A, \" + str(box_dimensions[2]) +\n",
        "  \" A.\\nGiving an aproximate volume of: \" + str(volume) + \" A\\u00b3\\n\")\n",
        "\n",
        "\n",
        "print(\"Calculating the exposure of each residue in the structure...\")\n",
        "RADIUS = 16.0\n",
        "RADIUS = 16.0\n",
        "hse_cb = HSExposureCB(mdel, radius=RADIUS)\n",
        "buried = []\n",
        "semiburied = []\n",
        "exposed = []\n",
        "print(\"Sorting the residues depending on their exposure...\")\n",
        "for r in mdel.get_residues():\n",
        "    if is_aa(r) and r.xtra[\"EXP_HSE_B_U\"] >= 35 and r.xtra[\"EXP_HSE_B_D\"] >= 35:\n",
        "        buried.append(r)\n",
        "    elif is_aa(r) and 35 > r.xtra[\"EXP_HSE_B_U\"] >= 25 and 35 > r.xtra[\"EXP_HSE_B_D\"] >= 25:\n",
        "        semiburied.append(r)\n",
        "    elif is_aa(r):\n",
        "        exposed.append(r)\n",
        "\n",
        "# Calculate the surface in A according to the predicted surface of each amino acid classified as exposed\n",
        "# - 10.1371/journal.pone.0080635\n",
        "aa = [\"ALA\", \"ARG\", \"ASN\", \"ASP\", \"CYS\", \"GLU\", \"GLN\", \"GLY\", \"HIS\", \"ILE\", \"LEU\", \"LYS\", \"MET\", \"PHE\", \"PRO\",\n",
        "  \"SER\", \"THR\", \"TRP\", \"TYR\", \"VAL\"]\n",
        "aa_s2 = {}\n",
        "aa_n = {}\n",
        "for aaname in aa:\n",
        "    aa_s2.update({aaname: int()})\n",
        "    aa_n.update({aaname: int()})\n",
        "\n",
        "aa_ASA = {\"ALA\": 121, \"ARG\": 265, \"ASN\": 187, \"ASP\": 187, \"CYS\": 148, \"GLU\": 214, \"GLN\": 214, \"GLY\": 97,\n",
        "      \"HIS\": 216, \"ILE\": 195, \"LEU\": 191, \"LYS\": 230, \"MET\": 203, \"PHE\": 228, \"PRO\": 154, \"SER\": 143,\n",
        "      \"THR\": 163, \"TRP\": 265, \"TYR\": 255, \"VAL\": 165}\n",
        "totals2 = 0\n",
        "for r in exposed:\n",
        "    totals2 += aa_ASA[r.get_resname()]\n",
        "i = 0\n",
        "for i in range(20):\n",
        "    if r.get_resname() == aa[i]:\n",
        "        aa_s2[aa[i]] += aa_ASA[aa[i]]\n",
        "        aa_n[aa[i]] += 1\n",
        "    else:\n",
        "        i += 1\n",
        "\n",
        "aa_s2_perc = {}\n",
        "for aaname in aa:\n",
        "    aa_s2_perc.update({aaname: str(\"{:.2f}\".format(((aa_s2[aaname] / totals2) * 100)) + \"%\")})\n",
        "\n",
        "print(\"Total number of surface residues: \" + str(len(exposed)) + \" out of \" + str(len(exposed) + len(semiburied) + len(buried)))\n",
        "\n",
        "if os.path.exists(\"General_info.txt\"):\n",
        "    check = open(\"General_info.txt\").read()\n",
        "else:\n",
        "    check = \"\"\n",
        "if \"As for\" not in check:\n",
        "    with open(\"General_info.txt\", \"a\") as f1:\n",
        "        print(\"Your protein aproximate dimensions in Angstroms are: \" + str(box_dimensions[0]) + \"A, \" +\n",
        "              str(box_dimensions[1]) + \"A, \" + str(\n",
        "            box_dimensions[2]) + \"A.\\nWhich makes an aproximate volume of: \" +\n",
        "              str(volume) + \" A\\u00b3\\\\n\", file=f1)\n",
        "        print(\"\\nAs for its surface, it measures \" + str(totals2) + \" A\\u00b2\\\\n\", file=f1)\n",
        "        print(\"From which each amino acid contributes to:\", file=f1)\n",
        "        print(\"Aminoacid\\t Surface(%)\\t Total number\", file=f1)\n",
        "        for aa in aa_s2_perc:\n",
        "            print(aa + \"\\t\\t\" + aa_s2_perc[aa] + \"\\t\\t\" + str(aa_n[aa]), file=f1)\n",
        "        print(\"Total number of surface residues: \" + str(len(exposed)), file=f1)\n",
        "\n",
        "\n",
        "aatocheck = [\"LYS\", \"GLU\", \"ASP\", \"HIS\", \"CYS\", \"TYR\", \"ARG\"]  # commonly used for immobilisation#\n",
        "hydrophobic = [\"ILE\", \"LEU\", \"PHE\", \"VAL\", \"TRP\", \"TYR\"]  # arunachalam 2008#\n",
        "charged = []\n",
        "hydroph = []\n",
        "\n",
        "for r in exposed:\n",
        "    name = r.get_resname()\n",
        "    if name in aatocheck:\n",
        "        chainid = r.get_parent().get_id()\n",
        "        resseq = r.get_id()[1]\n",
        "        charged.append(name + \"_\" + str(resseq) + \"_\" + chainid)\n",
        "    elif name in hydrophobic:\n",
        "        chainid = r.get_parent().get_id()\n",
        "        resseq = r.get_id()[1]\n",
        "        hydroph.append(name + \"_\" + str(resseq) + \"_\" + chainid)\n",
        "\n",
        "poslist = []\n",
        "neglist = []\n",
        "hislist = []\n",
        "lyslist = []\n",
        "cyslist = []\n",
        "\n",
        "for item in charged:\n",
        "    if \"LYS\" in item:\n",
        "        poslist.append(item)\n",
        "        lyslist.append(item)\n",
        "    elif \"ASP\" in item or \"GLU\" in item:\n",
        "        neglist.append(item)\n",
        "    elif \"HIS\" in item:\n",
        "        poslist.append(item)\n",
        "        hislist.append(item)\n",
        "    elif \"CYS\" in item:\n",
        "        cyslist.append(item)\n",
        "    elif \"ARG\" in item:\n",
        "        poslist.append(item)\n",
        "\n",
        "def clustering(list1, dic):\n",
        "    # This method checks if the residues in a certain list have their CA at less than 10A.\n",
        "    # If so, this residues will be included in the dictionary as a new cluster\n",
        "    full_structure = parser.get_structure(fullid, fullfile)\n",
        "    i = 0\n",
        "    while i in range(len(list1)):\n",
        "        x1 = list1[i]\n",
        "        for x2 in list1:\n",
        "            if x1 != x2:\n",
        "                dis = mdel[x1.split(\"_\")[2]][int(x1.split(\"_\")[1])][\"CA\"] - \\\n",
        "                      mdel[x2.split(\"_\")[2]][int(x2.split(\"_\")[1])][\"CA\"]\n",
        "                if dis < 10:\n",
        "                    try:\n",
        "                        dic[x1].append(x2)\n",
        "                    except KeyError:\n",
        "                        dic[x1] = [x2]\n",
        "        i += 1\n",
        "\n",
        "def clean(dict1):\n",
        "    # Clean up the dictionaries created with clustering. This avoids repetition of the same cluster as well as\n",
        "    # deletes any cluster formed by less than 3 different aminoacids\n",
        "    dictcopy = dict1.copy()\n",
        "    keys = list(dictcopy.keys())\n",
        "    for key in keys:\n",
        "        if key in dict1[key]:\n",
        "            dict1.pop(key)\n",
        "        elif len(dict1[key]) < 2:\n",
        "            dict1.pop(key)\n",
        "        elif key in str(dict1.values()):\n",
        "            dict1.pop(key)\n",
        "        elif \"\" in dict1[key]:\n",
        "            while \"\" in dict1[key]:\n",
        "                dict1[key].remove(\"\")\n",
        "\n",
        "def writeclusterstocsv(cluster, file):\n",
        "    # Self explanatory. Writes the clusters identified into a csv file.\n",
        "    with open(file, \"a\", newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if len(cluster) > 1:\n",
        "            for i in cluster:\n",
        "                towrite = [i]\n",
        "                for x in cluster[i]:\n",
        "                    towrite.append(x)\n",
        "                writer.writerow(towrite)\n",
        "            writer.writerow(\"\\n\")\n",
        "\n",
        "def show_clusters(dict):\n",
        "    # Writes the information of teh clusters into a txt file. More \"user friendly\" explanation of the results\n",
        "    f = open(\"Clusters.txt\", \"a\")\n",
        "    if len(dict) > 1:\n",
        "        f.write(list(dict.keys())[0] + \" residues in a cluster (<10A):\\n\")\n",
        "        i = 1\n",
        "        for keys in list(dict.keys())[1:]:\n",
        "            toprint = \"Cluster \" + str(list(dict.keys())[0][0:3].lower()) + str(i) + \":\" + str(keys)\n",
        "            length = len(dict[keys])\n",
        "            x = 0\n",
        "            while x < length:\n",
        "                toprint += \", \" + str(dict[keys][x])\n",
        "                x += 1\n",
        "            toprint += \".\"\n",
        "            print(toprint, file=f)\n",
        "            i += 1\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "def uploadcluster():\n",
        "    # In case the clusters.csv (which is created in the first execution -  is available, file is opened and clusters\n",
        "    # written back into a Python dictionary to continue execution\n",
        "    csv_clusters = csv.reader(open(\"clusters.csv\", \"r\"))\n",
        "    i = 0\n",
        "    keys = [[], [], [], [], [], []]\n",
        "    values = [[], [], [], [], [], []]\n",
        "    with open(\"clusters.csv\", \"r\") as file:\n",
        "        csv_clusters = csv.reader(file)\n",
        "        for row in csv_clusters:\n",
        "            if row[0] == \"\\n\":\n",
        "                i += 1\n",
        "            else:\n",
        "                keys[i].append(row[0])\n",
        "                values[i].append(row[1:])\n",
        "    x = 0\n",
        "    for x in range(len(keys)):\n",
        "        try:\n",
        "            if keys[x][0] == \"Positive\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_pos[keys[0][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Negative\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_neg[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Histidine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_his[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Lysine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_lys[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Cysteine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_cys[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Hydrophobic\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_hidroph[keys[x][y]] = values[x][y]\n",
        "        except IndexError:\n",
        "            x += 1\n",
        "        x += 1\n",
        "def clusters_pymol(dict):\n",
        "    # Writes the clusters into a pymol command which can be direclty copied into the command line\n",
        "    if len(dict) > 1:\n",
        "        f = open(\"PyMol_clusters.pml\", \"a\")\n",
        "        colours = coloursdic[list(dict.keys())[0]]\n",
        "        i = 0\n",
        "        for keys in dict.keys():\n",
        "            if keys.isalpha() is not True:\n",
        "                toprint = (\"select \" + str(list(dict.keys())[0][0:3].lower()) + str(i) + \", (resi \" + str(\n",
        "                    keys.split(\"_\")[1])) + \" & chain \" + str(dict[keys][1].split(\"_\")[2])\n",
        "                length = len(dict[keys])\n",
        "                x = 0\n",
        "                while x < length:\n",
        "                    toprint += \", resi \" + str(dict[keys][x].split(\"_\")[1]) + \" & chain \" + str(\n",
        "                        dict[keys][x].split(\"_\")[2])\n",
        "                    x += 1\n",
        "                toprint += \")\"\n",
        "                print(toprint, file=f)\n",
        "            i += 1\n",
        "        print(\"color \" + colours + \", \" + str(list(dict.keys())[0][0:3].lower()) + \"*\\n\", file=f)\n",
        "\n",
        "\n",
        "cluster_pos = {\"Positive\": [\"Lys\", \"Arg\", \"His\"]}\n",
        "cluster_neg = {\"Negative\": [\"Asp\", \"Gly\"]}\n",
        "cluster_his = {\"Histidine\": [\"only\", \"histidines\"]}\n",
        "cluster_lys = {\"Lysine\": [\"only\", \"lysines\"]}\n",
        "cluster_cys = {\"Cysteine\": [\"only\", \"cysteines\"]}\n",
        "cluster_hidroph = {\"Hydrophobic\": [\"ILE\", \"LEU\", \"PHE\", \"VAL\", \"TRP\", \"TYR\"]}\n",
        "coloursdic = {\"Positive\": \"Blue\", \"Negative\": \"Red\", \"Histidine\": \"Cyan\", \"Lysine\": \"LightBlue\",\n",
        "              \"Cysteine\": \"Yellow\", \"Hydrophobic\": \"White\"}\n",
        "\n",
        "if os.path.exists(\"clusters.csv\") is True:\n",
        "    print(\"Uploading previously calculated clusters form clusters.csv.\\n\")\n",
        "    uploadcluster()\n",
        "elif os.path.exists(\"clusters.csv\") is False:\n",
        "    print(\"Measuring distances and identifying clusters...\")\n",
        "    clustering(poslist, cluster_pos)\n",
        "    clustering(neglist, cluster_neg)\n",
        "    clustering(hislist, cluster_his)\n",
        "    clustering(lyslist, cluster_lys)\n",
        "    clustering(cyslist, cluster_cys)\n",
        "    clustering(hydroph, cluster_hidroph)\n",
        "    clean(cluster_pos)\n",
        "    clean(cluster_neg)\n",
        "    clean(cluster_his)\n",
        "    clean(cluster_lys)\n",
        "    clean(cluster_cys)\n",
        "    clean(cluster_hidroph)\n",
        "    writeclusterstocsv(cluster_pos, \"clusters.csv\")\n",
        "    writeclusterstocsv(cluster_neg, \"clusters.csv\")\n",
        "    writeclusterstocsv(cluster_his, \"clusters.csv\")\n",
        "    writeclusterstocsv(cluster_lys, \"clusters.csv\")\n",
        "    writeclusterstocsv(cluster_cys, \"clusters.csv\")\n",
        "    writeclusterstocsv(cluster_hidroph, \"clusters.csv\")\n",
        "    show_clusters(cluster_pos)\n",
        "    show_clusters(cluster_neg)\n",
        "    show_clusters(cluster_his)\n",
        "    show_clusters(cluster_lys)\n",
        "    show_clusters(cluster_cys)\n",
        "    show_clusters(cluster_hidroph)\n",
        "    print(\"Clusters can be found in Clusters.txt\")\n",
        "\n",
        "with open(\"PyMol_clusters.pml\", \"w\") as f:\n",
        "    chainid = []\n",
        "    for chains in full_structure.get_chains():\n",
        "        if chains.get_id() not in chainid:\n",
        "            chainid.append(chains.get_id())\n",
        "    if len(chains) > 1:\n",
        "        for chain in chainid:\n",
        "            print(\"set_color shade\" + str(chain) + \"= [1.0, \" + str(random.random()) + \", \" + str(\n",
        "                random.random()) + \"]\", file=f)\n",
        "            print(\"colour shade\" + str(chain) + \", chain \" + str(chain), file=f)\n",
        "clusters_pymol(cluster_pos)\n",
        "clusters_pymol(cluster_neg)\n",
        "clusters_pymol(cluster_his)\n",
        "clusters_pymol(cluster_lys)\n",
        "clusters_pymol(cluster_cys)\n",
        "clusters_pymol(cluster_hidroph)\n",
        "\n",
        "def distointer(cluster, list1):\n",
        "    # Check if the residues in the cluser can interfere with the protein interface\n",
        "    itemslist = []\n",
        "    for kcluster in list(cluster.keys())[1:]:\n",
        "        itemslist.append(kcluster)\n",
        "        for values in cluster[kcluster]:\n",
        "            itemslist.append(values)\n",
        "    for pos1 in itemslist:\n",
        "        for pos2 in interface_res[\"A\"]:\n",
        "            dis = mdel[str(pos1.split(\"_\")[2])][int(pos1.split(\"_\")[1])][\"CA\"] - mdel[\"A\"][pos2][\"CA\"]\n",
        "            if dis != 0 and dis <= 10 and (pos1 not in list1):\n",
        "                list1.append(pos1)\n",
        "            if IndexError:\n",
        "                continue\n",
        "\n",
        "def writecluster(original, d1, list1, file):\n",
        "    # Similar to show_cluster method, it writes the information into a .txt file. In this case, it will add a\n",
        "    # -Warning- if the cluster is close to the specified residues.'''\n",
        "    it = 1\n",
        "    f = open(file, \"a\")\n",
        "    with open(file) as readfile:\n",
        "        if \"Chain\" not in readfile.read():\n",
        "            f.write(\"Searching for clusters in close contact to: \")\n",
        "            if isinstance(original, dict) is True:\n",
        "                for aas in original:\n",
        "                    f.write(\"\\nChain \" + str(aas) + \":\")\n",
        "                    for aas2 in original[aas]:\n",
        "                        f.write(\" - \" + str(aas2))\n",
        "            elif isinstance(original, list):\n",
        "                for aas in original:\n",
        "                    f.write(\" - \" + str(aas))\n",
        "            readfile.close()\n",
        "        else:\n",
        "            readfile.close()\n",
        "    f.write(\"\\n\" + list(d1.keys())[0] + \" residues in a cluster (<10A):\\n\")\n",
        "    inlist1 = []\n",
        "    for k in list(d1.keys())[1:]:\n",
        "        toprint = \"Cluster \" + str(list(d1.keys())[0][0:3].lower()) + str(it) + \":\" + str(k)\n",
        "        length = len(d1[k])\n",
        "        inlist1.append(k)\n",
        "        x = 0\n",
        "        while x < length:\n",
        "            toprint += \", \" + str(d1[k][x])\n",
        "            inlist1.append(d1[k][x])\n",
        "            x += 1\n",
        "        toprint += \".\"\n",
        "        for ex in inlist1:\n",
        "            if ex in list1 and \"\\t -In close proximity to specified residue/s - \" not in toprint:\n",
        "                toprint += \"\\t -In close proximity to specified residue/s - \"\n",
        "        print(toprint, file=f)\n",
        "        it += 1\n",
        "    f.write(\"\\n\")\n",
        "    f.close()\n",
        "\n",
        "# Start first, identifying how many chains does the model have. Maximum of 10 for convenience\n",
        "# (and because most proteins fall into this category).\n",
        "chainres = {\"A\": [], \"B\": [], \"C\": [], \"D\": [], \"E\": [], \"F\": [], \"G\": [], \"H\": [], \"I\": [], \"J\": []}\n",
        "chainids = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\"]\n",
        "\n",
        "# number of chains in model\n",
        "number_of_chains = 0\n",
        "for chain in mdel.get_chains():\n",
        "    number_of_chains += 1\n",
        "\n",
        "# Create dictionary with all CA location of a chain\n",
        "for x in range(number_of_chains):\n",
        "    for residues in mdel[chainids[x]]:\n",
        "        if is_aa(residues) is True:\n",
        "            chainres[chainids[x]].append(residues[\"CA\"])\n",
        "\n",
        "# Delete unnecessary keys in the chainres dictionary\n",
        "for a in range(len(chainres.keys())):\n",
        "    if len(chainres[chainids[a]]) == 0:\n",
        "        del chainres[chainids[a]]\n",
        "\n",
        "# calculation of the interface residues. This will assume all chains will have the same interactions as chain A.\n",
        "# This simplification is necessary to get results in a reasonable time.\n",
        "interface_res = {\"A\": []}\n",
        "\n",
        "if number_of_chains > 1:\n",
        "    print(\"Calculating if any of the clusters might affect the quaternary structure of your protein. \\n \"\n",
        "          \"Note: This assumes chain A is in contact with all other subunits and all contacts are identical \"\n",
        "          \"between subunits.\\n\")\n",
        "\n",
        "calculationdone = os.path.exists(\"residues_interface.txt\")\n",
        "\n",
        "print(\"Calculating contacts between chain A and it's neighbours...\")\n",
        "ch_1 = 1\n",
        "# Calculate the distance of all atoms in chain A to all atoms in other chains. If this distance is <10A,\n",
        "# the position will be added as part of the interface.\n",
        "while ch_1 < number_of_chains:\n",
        "    for ca1 in chainres[\"A\"]:\n",
        "        for ca2 in chainres[chainids[ch_1]]:\n",
        "            if ca1.get_parent().get_id()[1] not in interface_res[\"A\"]:\n",
        "                dist = ca1 - ca2\n",
        "                if 0 < dist < 10:\n",
        "                    interface_res[\"A\"].append(ca1.get_parent().get_id()[1])\n",
        "    ch_1 += 1\n",
        "\n",
        "    # Assuming all contacts in the different chains are the same as in chain A,\n",
        "    # copy the residue position to each chain.\n",
        "    count_start = 1\n",
        "    for count_start in range(number_of_chains):\n",
        "        interface_res[chainids[count_start]] = interface_res[\"A\"]\n",
        "        count_start += 1\n",
        "\n",
        "    # Write information to file so it doesn't have to be calculated again.\n",
        "    with open(\"residues_interface.txt\", \"a\") as fileint:\n",
        "        print(interface_res, file=fileint)\n",
        "    print(\"The position of the residues identified as part of the interface can be found in residues_interface.txt\")\n",
        "\n",
        "# Calculate now if any of the identified clusters is at less than 10A to the any of the interface residues.\n",
        "intpos = []\n",
        "intneg = []\n",
        "inthis = []\n",
        "intlys = []\n",
        "intcys = []\n",
        "inthidroph = []\n",
        "\n",
        "distointer(cluster_pos, intpos)\n",
        "writecluster(interface_res, cluster_pos, intpos, \"Interface_contacts.txt\")\n",
        "distointer(cluster_neg, intneg)\n",
        "writecluster(interface_res, cluster_neg, intneg, \"Interface_contacts.txt\")\n",
        "distointer(cluster_his, inthis)\n",
        "writecluster(interface_res, cluster_his, inthis, \"Interface_contacts.txt\")\n",
        "distointer(cluster_lys, intlys)\n",
        "writecluster(interface_res, cluster_lys, intlys, \"Interface_contacts.txt\")\n",
        "distointer(cluster_cys, intcys)\n",
        "writecluster(interface_res, cluster_cys, intcys, \"Interface_contacts.txt\")\n",
        "distointer(cluster_hidroph, inthidroph)\n",
        "writecluster(interface_res, cluster_hidroph, inthidroph, \"Interface_contacts.txt\")\n",
        "print(\"Distance to the multimeric interface successful. Your results can be found in Cluster_interface.txt\")\n",
        "\n",
        "# __________________________ACTIVE SITE + CLUSTERS _________________________________________________________________\n",
        "#   \n",
        "def dist_to_as(dict1, list1):\n",
        "    # Similarly to the other distance methods, calculates if any residues of the cluster is at less than\n",
        "    # 10A of any of the residues defined as the active site'''\n",
        "    cluster_aa = []\n",
        "    for k in list(dict1.keys())[1:]:\n",
        "        cluster_aa.append(k)\n",
        "        for i in dict1[k]:\n",
        "            cluster_aa.append(i)\n",
        "    for as_site in aspositions:\n",
        "        for x in cluster_aa:\n",
        "            for i in range(number_of_chains):\n",
        "                dis = mdel[chainids[i]][int(as_site)][\"CA\"] - mdel[x.split(\"_\")[2]][int(x.split(\"_\")[1])][\"CA\"]\n",
        "                if dis < 5:\n",
        "                    if x not in list1:\n",
        "                        list1.append(x)\n",
        "\n",
        "if ACTIVE_SITE == True:\n",
        "    # If this information exists in the active_site directory, give the option to use directly that information.\n",
        "    # If not, ask for user input on where the active site is located\n",
        "    parent_dir = os.path.dirname(os.getcwd())\n",
        "    str_ac = parent_dir + \"/ActiveSite/Active_site.txt\"\n",
        "    aspositions_0 = []\n",
        "    if os.path.exists(str_ac) is True:\n",
        "        with open(parent_dir + \"/ActiveSite/Active_site.txt\", \"r\") as f:\n",
        "            for line in f.readlines():\n",
        "                if \"-->\" in line:\n",
        "                    toadd = re.findall(r'\\d+', line)\n",
        "                    aspositions_0.append(toadd)\n",
        "    else:\n",
        "        print(\"Cannot find the active site identified with the ActiveSiteID module, \"\n",
        "              \"please run it and check the results!\")\n",
        "    \n",
        "aspositions = []\n",
        "for aa in aspositions_0:\n",
        "    if isinstance(aa, list) is True:\n",
        "        for aa1 in aa:\n",
        "            aspositions.append(aa1)\n",
        "    else:\n",
        "        aspositions.append(aa)\n",
        "\n",
        "    # Calculate if any residue in the cluster is close to the active site residues\n",
        "asposd = []\n",
        "asnegd = []\n",
        "ashisd = []\n",
        "aslysd = []\n",
        "ascysd = []\n",
        "ashidrophd = []\n",
        "\n",
        "number_of_chains = 0\n",
        "for chain in mdel.get_chains():\n",
        "    number_of_chains += 1\n",
        "    \n",
        "dist_to_as(cluster_pos, asposd)\n",
        "dist_to_as(cluster_neg, asnegd)\n",
        "dist_to_as(cluster_his, ashisd)\n",
        "dist_to_as(cluster_lys, aslysd)\n",
        "dist_to_as(cluster_cys, ascysd)\n",
        "dist_to_as(cluster_hidroph, ashidrophd)\n",
        "writecluster(aspositions, cluster_pos, asposd, \"Cluster_ActiveSite.txt\")\n",
        "writecluster(aspositions, cluster_neg, asnegd, \"Cluster_ActiveSite.txt\")\n",
        "writecluster(aspositions, cluster_his, ashisd, \"Cluster_ActiveSite.txt\")\n",
        "writecluster(aspositions, cluster_lys, aslysd, \"Cluster_ActiveSite.txt\")\n",
        "writecluster(aspositions, cluster_cys, ascysd, \"Cluster_ActiveSite.txt\")\n",
        "writecluster(aspositions, cluster_hidroph, ashidrophd, \"Cluster_ActiveSite.txt\")\n",
        "print(\"Information of the cluster distance to the active site can be found in Cluster_ActiveSite.txt\")"
      ],
      "metadata": {
        "id": "W4e-193AjVyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Show 3D structure with the clusters\n",
        "SURFACE_FOLDER = \"/content/TEST/Surface\" #@param {type:\"string\"}\n",
        "#@markdown Indicate the full path to your Surface folder. Ex. /content/test/Surface\n",
        "FILE_CLUSTERS = SURFACE_FOLDER + '/clusters.csv'\n",
        "\n",
        "import py3Dmol\n",
        "\n",
        "def uploadcluster(FILE):\n",
        "    # In case the clusters.csv (which is created in the first execution -  is available, file is opened and clusters\n",
        "    # written back into a Python dictionary to continue execution\n",
        "    csv_clusters = csv.reader(open(FILE, \"r\"))\n",
        "    i = 0\n",
        "    cluster_pos, cluster_neg, cluster_his, cluster_lys, cluster_cys, cluster_hidroph = {}, {} ,{} ,{} ,{} ,{}\n",
        "    keys = [[], [], [], [], [], []]\n",
        "    values = [[], [], [], [], [], []]\n",
        "    with open(FILE, \"r\") as file:\n",
        "        csv_clusters = csv.reader(file)\n",
        "        for row in csv_clusters:\n",
        "            if row[0] == \"\\n\":\n",
        "                i += 1\n",
        "            else:\n",
        "                keys[i].append(row[0])\n",
        "                values[i].append(row[1:])\n",
        "\n",
        "    for x in range(len(keys)):\n",
        "        try:\n",
        "            if keys[x][0] == \"Positive\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_pos[keys[0][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Negative\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_neg[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Histidine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_his[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Lysine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_lys[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Cysteine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_cys[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Hydrophobic\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_hidroph[keys[x][y]] = values[x][y]\n",
        "        except IndexError:\n",
        "            x += 1\n",
        "        x += 1\n",
        "    return cluster_pos, cluster_neg, cluster_his, cluster_lys, cluster_cys, cluster_hidroph\n",
        "\n",
        "\n",
        "def num_cluster(DICT):\n",
        "  list1 = []\n",
        "  for k in list(DICT.keys()):\n",
        "    if '_' in k:\n",
        "      list1.append(k.split('_')[1])\n",
        "  list2 = []\n",
        "  for v_list in list(DICT.values()):\n",
        "      for v in v_list:\n",
        "          if '_' in v:\n",
        "              list2.append(v.split('_')[1])\n",
        "  list3 = list1 + list2\n",
        "  return list3\n",
        "\n",
        "\n",
        "\n",
        "if os.path.exists(FILE_CLUSTERS) is True:\n",
        "    r_cluster_pos, r_cluster_neg, r_cluster_his, r_cluster_lys, r_cluster_cys, r_cluster_hidrop = uploadcluster(FILE_CLUSTERS)\n",
        "    cluster_pos = num_cluster(r_cluster_pos)\n",
        "    cluster_neg = num_cluster(r_cluster_neg)\n",
        "    cluster_his = num_cluster(r_cluster_his)\n",
        "    cluster_lys = num_cluster(r_cluster_lys)\n",
        "    cluster_cys = num_cluster(r_cluster_cys)\n",
        "    cluster_hidrop = num_cluster(r_cluster_hidrop)\n",
        "elif os.path.exists(FILE_CLUSTERS) is False:\n",
        "    print('Cannot find the clutsers.csv file.. make sure you pointed to the correct path!')\n",
        "\n",
        "import py3Dmol\n",
        "view = py3Dmol.view()\n",
        "try:\n",
        "    view.addModel(open(SURFACE_FOLDER + '/query.pdb','r').read(),'pdb')\n",
        "except FileNotFoundError:\n",
        "    print('There is no pdb file in the Surface folder... and it should be named query.pdb!')\n",
        "\n",
        "view.setStyle({'model':-1},{'cartoon': {'color':'white'}})\n",
        "view.setBackgroundColor('black')\n",
        "view.setStyle({'resi': cluster_pos}, {\"sphere\": {'color': 'blue'}})\n",
        "view.setStyle({'resi': cluster_neg}, {\"sphere\": {'color': 'red'}})\n",
        "view.setStyle({'resi': cluster_his}, {\"sphere\": {'color': 'cyan'}})\n",
        "view.setStyle({'resi': cluster_lys}, {\"sphere\": {'color': 'lightblue'}})\n",
        "view.setStyle({'resi': cluster_cys}, {\"sphere\": {'color': 'yellow'}})\n",
        "view.setStyle({'resi': cluster_hidrop}, {\"sphere\": {'color': 'white'}})\n",
        "\n",
        "view.zoomTo()\n",
        "view.show()"
      ],
      "metadata": {
        "id": "LeEwshu61iJ-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MODULE 3.1- CLUSTER DISTANCE\n",
        "#@markdown Determine the distance between any user defined residue in your structure and the different clusters.\n",
        "\n",
        "#@markdown Indicate the full path to the Surface folder where the information is.\n",
        "SURFACE_FOLDER = \"/content/TEST/Surface\" #@param {type:\"string\"}\n",
        "#@markdown Enter the residues to calculate distance to with the number and the chain and separated by a comma. Ex. 1_A, 200_B\n",
        "RESIDUES =  '50_A' #@param {type:\"string\"}\n",
        "#@markdown Enter the file name to save your results. \n",
        "FILE_NAME =  'test1' #@param {type:\"string\"}\n",
        "\n",
        "%cd $SURFACE_FOLDER\n",
        "import py3Dmol\n",
        "\n",
        "residues = RESIDUES.split(',')\n",
        "pdbfile = SURFACE_FOLDER + '/query.pdb'\n",
        "\n",
        "# Open the desired pdb file\n",
        "parser = PDBParser()\n",
        "fullid = \"Q00F\"\n",
        "pdbfiles = []\n",
        "fullfile = pdbfile\n",
        "full_structure = parser.get_structure(fullid, fullfile)\n",
        "mdel = full_structure[0]\n",
        "ppb = PPBuilder()\n",
        "\n",
        "def uploadcluster(FILE):\n",
        "    # In case the clusters.csv (which is created in the first execution -  is available, file is opened and clusters\n",
        "    # written back into a Python dictionary to continue execution\n",
        "    i = 0\n",
        "    cluster_pos, cluster_neg, cluster_his, cluster_lys, cluster_cys, cluster_hidroph = {}, {} ,{} ,{} ,{} ,{}\n",
        "    keys = [[], [], [], [], [], []]\n",
        "    values = [[], [], [], [], [], []]\n",
        "    with open(FILE, \"r\") as file:\n",
        "        csv_clusters = csv.reader(file)\n",
        "        for row in csv_clusters:\n",
        "            if row[0] == \"\\n\":\n",
        "                i += 1\n",
        "            else:\n",
        "                keys[i].append(row[0])\n",
        "                values[i].append(row[1:])\n",
        "    x = 0\n",
        "    for x in range(len(keys)):\n",
        "        try:\n",
        "            if keys[x][0] == \"Positive\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_pos[keys[0][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Negative\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_neg[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Histidine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_his[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Lysine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_lys[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Cysteine\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_cys[keys[x][y]] = values[x][y]\n",
        "            elif keys[x][0] == \"Hydrophobic\":\n",
        "                for y in range(len(keys[x])):\n",
        "                    cluster_hidroph[keys[x][y]] = values[x][y]\n",
        "        except IndexError:\n",
        "            x += 1\n",
        "        x += 1\n",
        "    return cluster_pos, cluster_neg, cluster_his,cluster_lys, cluster_cys, cluster_hidroph\n",
        "def num_cluster(DICT):\n",
        "    list1 = []\n",
        "    for k in list(DICT.keys()):\n",
        "      if '_' in k:\n",
        "        list1.append(k.split('_')[1])\n",
        "    list2 = []\n",
        "    for v_list in list(DICT.values()):\n",
        "        for v in v_list:\n",
        "            if '_' in v:\n",
        "                list2.append(v.split('_')[1])\n",
        "    list3 = list1 + list2\n",
        "    return list3\n",
        "\n",
        "\n",
        "def distonrl(cluster):\n",
        "    # Similarly to the other distance methods, calculates if any residues of the cluster is at less than 10A\n",
        "    # of any of the residues defined by the user\n",
        "    itemslist = []\n",
        "    list1=[]\n",
        "    for k0 in list(cluster.keys())[1:]:\n",
        "        itemslist.append(k0)\n",
        "        for values in cluster[k0]:\n",
        "            itemslist.append(values)\n",
        "    for i1 in itemslist:\n",
        "        for k1 in nrl_dic.keys():\n",
        "            for v0 in nrl_dic[k1]:\n",
        "                dis = mdel[str(i1.split(\"_\")[2])][int(i1.split(\"_\")[1])][\"CA\"] - mdel[str(k1)][int(v0)][\"CA\"]\n",
        "                if dis != 0 and dis < 10 and i1 not in list1:\n",
        "                    list1.append(i1)\n",
        "                elif IndexError:\n",
        "                    continue\n",
        "    return list1\n",
        "\n",
        "def writecluster(original, d1, list1, file):\n",
        "    # Writes the information into a .txt file. In this case, it will add a -Warning- if the cluster is close\n",
        "    # to the specified residues.\n",
        "    if len(d1) > 0:\n",
        "        it = 1\n",
        "        f = open(file, \"a\")\n",
        "        with open(file) as readfile:\n",
        "            if \"Chain\" not in readfile.read():\n",
        "                f.write(\"Searching for clusters in close contact to: \")\n",
        "                if isinstance(original, dict) is True:\n",
        "                    for aas in original:\n",
        "                        f.write(\"\\nChain \" + str(aas) + \":\")\n",
        "                        for aas2 in original[aas]:\n",
        "                            f.write(\" - \" + str(aas2))\n",
        "                elif isinstance(original, list):\n",
        "                    for aas in original:\n",
        "                        f.write(\" - \" + str(aas))\n",
        "                readfile.close()\n",
        "            else:\n",
        "                readfile.close()\n",
        "        f.write(\"\\n\" + list(d1.keys())[0] + \" residues in a cluster (<10A):\\n\")\n",
        "        inlist1 = []\n",
        "        for k in list(d1.keys())[1:]:\n",
        "            toprint = \"Cluster \" + str(list(d1.keys())[0][0:3].lower()) + str(it) + \":\" + str(k)\n",
        "            length = len(d1[k])\n",
        "            inlist1.append(k)\n",
        "            x = 0\n",
        "            while x < length:\n",
        "                toprint += \", \" + str(d1[k][x])\n",
        "                inlist1.append(d1[k][x])\n",
        "                x += 1\n",
        "            toprint += \".\"\n",
        "            for ex in inlist1:\n",
        "                if ex in list1 and \"\\t -In close proximity to specified residue/s - \" not in toprint:\n",
        "                    toprint += \"\\t -In close proximity to specified residue/s - \"\n",
        "            print(toprint, file=f)\n",
        "            it += 1\n",
        "        f.write(\"\\n\")\n",
        "        f.close() \n",
        "\n",
        "\n",
        "def distance_pml(dict, file):\n",
        "    filename= FILE_NAME + '.pml'\n",
        "    # Writes the clusters into a pymol command which can be directly copied into the command line\n",
        "    for key in dict.keys():\n",
        "        writing = 'select res_group ('\n",
        "        j=0\n",
        "        item_n = 0\n",
        "        for aa_n in dict[key]:\n",
        "            length = len(dict[key])\n",
        "            if item_n == 0:\n",
        "                writing += 'resi ' + aa_n + ' & chain ' + key\n",
        "                item_n += 1\n",
        "            else:\n",
        "                writing += ', resi ' + aa_n + ' & chain ' + key\n",
        "                item_n += 1\n",
        "        writing += ')\\n'\n",
        "        writing += 'zoom (res_group)'\n",
        "        with open(filename, \"w\") as f:\n",
        "            f.write(writing)\n",
        "    return filename\n",
        "\n",
        "\n",
        "# Extract from the pdb file the chain id and the sequence of each chain saved into a dictionary.\n",
        "\n",
        "cluster_pos, cluster_neg, cluster_his,cluster_lys, cluster_cys, cluster_hidrop = uploadcluster(SURFACE_FOLDER + '/clusters.csv')\n",
        "num_cluster_pos = num_cluster(cluster_pos)\n",
        "num_cluster_neg = num_cluster(cluster_neg)\n",
        "num_cluster_his = num_cluster(cluster_his)\n",
        "num_cluster_lys = num_cluster(cluster_lys)\n",
        "num_cluster_cys = num_cluster(cluster_cys)\n",
        "num_cluster_hidrop = num_cluster(cluster_hidroph)\n",
        "\n",
        "c_ids = {}\n",
        "for chains in mdel:\n",
        "    c_ids[chains.get_id()] = []\n",
        "for chains in c_ids:\n",
        "    c_ids[chains] = [ppb.build_peptides(mdel[chains])[0][0].get_id()[1],\n",
        "                     ppb.build_peptides(mdel[chains])[0][-1].get_id()[1]]\n",
        "\n",
        "# Ask user for the specific residues to check. Check that the specified residues exist in the pdb file.\n",
        "# nrl = new residues list, but I wanted to keep it short.\n",
        "nrl = residues\n",
        "for res in nrl:\n",
        "    if res.split(\"_\")[1] not in c_ids.keys():\n",
        "        print(\"\\nIt seems that your input is part of an unexistent chain.\")\n",
        "        quit()\n",
        "    elif int(res.split(\"_\")[0]) not in range(c_ids[res.split(\"_\")[1]][0], c_ids[res.split(\"_\")[1]][1]):\n",
        "        print(\"\\nIt seems that your input is not part of the pdb file.\")\n",
        "        quit()\n",
        "\n",
        "nrl_dic = {}\n",
        "for posit in nrl:\n",
        "    if posit.split(\"_\")[1] in nrl_dic.keys():\n",
        "        nrl_dic[posit.split(\"_\")[1]].append(posit.split(\"_\")[0])\n",
        "    else:\n",
        "        nrl_dic[posit.split(\"_\")[1]] = [posit.split(\"_\")[0]]\n",
        "\n",
        "perspos = distonrl(cluster_pos)\n",
        "persneg = distonrl(cluster_neg)\n",
        "pershis = distonrl(cluster_his)\n",
        "perslys = distonrl(cluster_lys)\n",
        "perscys = distonrl(cluster_cys)\n",
        "pershidroph = distonrl(cluster_hidroph)\n",
        "\n",
        "spfile = FILE_NAME\n",
        "\n",
        "writecluster(nrl_dic, cluster_pos, perspos, spfile + \".txt\")\n",
        "writecluster(nrl_dic, cluster_neg, persneg, spfile + \".txt\")\n",
        "writecluster(nrl_dic, cluster_his, pershis, spfile + \".txt\")\n",
        "writecluster(nrl_dic, cluster_lys, perslys, spfile + \".txt\")\n",
        "writecluster(nrl_dic, cluster_cys, perscys, spfile + \".txt\")\n",
        "writecluster(nrl_dic, cluster_hidrop, pershidroph, spfile + \".txt\")\n",
        "\n",
        "num_dist_pos = [x.split('_')[1] for x in perspos]\n",
        "num_dist_neg = [x.split('_')[1] for x in persneg]\n",
        "num_dist_his = [x.split('_')[1] for x in pershis]\n",
        "num_dist_lys = [x.split('_')[1] for x in perslys]\n",
        "num_dist_cys = [x.split('_')[1] for x in perscys]\n",
        "num_dist_hidrop = [x.split('_')[1] for x in pershidroph]\n",
        "\n",
        "contact_list = num_dist_pos + num_dist_his + num_dist_lys + num_dist_cys + num_dist_hidrop\n",
        "\n",
        "pml_file = distance_pml(nrl_dic, FILE_NAME)\n",
        "\n",
        "nrl_list = [x.split('_')[0] for x in nrl]\n",
        "import py3Dmol\n",
        "view = py3Dmol.view()\n",
        "try:\n",
        "    view.addModel(open(SURFACE_FOLDER + '/query.pdb','r').read(),'pdb')\n",
        "except FileNotFoundError:\n",
        "    print('There is no pdb file in the Surface folder... and it should be named query.pdb!')\n",
        "\n",
        "view.setStyle({'model':-1},{'cartoon': {'color':'white'}})\n",
        "view.setBackgroundColor('black')\n",
        "view.setStyle({'resi': num_cluster_pos}, {\"sphere\": {'color': 'blue'}})\n",
        "view.setStyle({'resi': num_cluster_neg}, {\"sphere\": {'color': 'red'}})\n",
        "view.setStyle({'resi': num_cluster_his}, {\"sphere\": {'color': 'cyan'}})\n",
        "view.setStyle({'resi': num_cluster_lys}, {\"sphere\": {'color': 'lightblue'}})\n",
        "view.setStyle({'resi': num_cluster_cys}, {\"sphere\": {'color': 'yellow'}})\n",
        "view.setStyle({'resi': num_dist_hidrop}, {\"sphere\": {'color': 'white'}})\n",
        "view.setStyle({'resi': contact_list}, {\"sphere\": {'color': 'magenta'}})\n",
        "view.setStyle({'resi': nrl_list}, {\"sphere\": {'color': 'purple'}})\n",
        "\n",
        "\n",
        "view.zoomTo()\n",
        "view.show()\n"
      ],
      "metadata": {
        "id": "egE10AK8jH3l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download your results:\n",
        "#@markdown Just indicate your project folder with the full path (Ex. /content/TEST) to donwload all results in a zip file. \n",
        "PROJECT = \"TEST\" #@param {type:\"string\"}\n",
        "\n",
        "os.chdir('/content/')\n",
        "PROJECT_NAME = PROJECT.split('/')[-1] + '.gz.tar'\n",
        "\n",
        "!tar -zcf $PROJECT_NAME $PROJECT\n",
        "\n",
        "from google.colab import files\n",
        "files.download(PROJECT_NAME) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6-2nmKglBj7M",
        "outputId": "4dcd7ae8-500c-4286-c8f0-03bfa626cb83",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b4eb2719-6eac-42bd-811b-d1989e91c648\", \"TEST.gz.tar\", 1421768)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}